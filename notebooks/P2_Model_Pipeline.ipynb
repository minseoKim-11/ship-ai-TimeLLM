{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. 경로 설정 ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b240ce5-2394-46d5-84b8-9b95150cd6cc",
   "metadata": {},
   "source": [
    "## Phase2-A : Data Handler 정의\n",
    "\n",
    "12개 채널의 Scale 일치를 위해 Standardization을 진행합니다.\n",
    "<br>모든 12개 채널의 Scale을평균 0, 표준편차 1인 표준 정규 분포로 통일시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a767163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# [1] --- DataHandler 클래스 정의 (V2: Scaler + zfill) ---\n",
    "# (Phase 2-A와 2-C가 합쳐진 최종 버전)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. 경로 설정 ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    [V2] 표준화(Standardization)와 zfill(6)이 적용된 DataHandler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, train_end_date='2022-12-31'):\n",
    "        self.file_path = file_path\n",
    "        self.train_end_date = pd.to_datetime(train_end_date)\n",
    "        self.data_by_ticker = {}   # 원본 데이터\n",
    "        self.scalers_by_ticker = {} # Ticker별 Scaler\n",
    "        self.tickers = []\n",
    "        \n",
    "        self._load_and_process_data()\n",
    "        self._fit_scalers()\n",
    "        \n",
    "    def _load_and_process_data(self):\n",
    "        try:\n",
    "            # 1. dtype=str로 읽기\n",
    "            df = pd.read_csv(\n",
    "                self.file_path, \n",
    "                parse_dates=['date'],\n",
    "                dtype={'ticker': str} \n",
    "            )\n",
    "            # 2. zfill(6)로 '0' 채우기\n",
    "            df['ticker'] = df['ticker'].str.zfill(6)\n",
    "            df = df.set_index('date')\n",
    "            \n",
    "            self.tickers = df['ticker'].unique()\n",
    "            \n",
    "            for ticker in self.tickers:\n",
    "                ticker_df = df[df['ticker'] == ticker].copy()\n",
    "                channel_cols = [col for col in ticker_df.columns if col not in ['ticker']]\n",
    "                self.data_by_ticker[ticker] = ticker_df[channel_cols]\n",
    "            \n",
    "            print(f\"[DataHandler V2] Success: Loaded {len(self.tickers)} tickers.\")\n",
    "            print(f\"[DataHandler V2] Available tickers: {self.tickers}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DataHandler V2] Error loading data: {e}\")\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        [Data Leakage 방지] 훈련 데이터로만 Scaler를 학습(fit)\n",
    "        \"\"\"\n",
    "        print(f\"[DataHandler V2] Fitting scalers using data up to {self.train_end_date.date()}...\")\n",
    "        for ticker in self.tickers:\n",
    "            train_data = self.data_by_ticker[ticker].loc[:self.train_end_date]\n",
    "            if train_data.empty:\n",
    "                print(f\"  > Warning: No training data for {ticker}.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_data) # 'fit'은 훈련 데이터로만!\n",
    "            self.scalers_by_ticker[ticker] = scaler\n",
    "        print(\"[DataHandler V2] Scalers fitted.\")\n",
    "\n",
    "    def get_scaled_data_by_ticker(self, ticker):\n",
    "        \"\"\"\n",
    "        'transform'은 전체 데이터에 적용하여 표준화된 DF 반환\n",
    "        \"\"\"\n",
    "        if ticker not in self.scalers_by_ticker:\n",
    "            print(f\"[DataHandler V2] Error: No scaler for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        original_data = self.data_by_ticker[ticker]\n",
    "        scaler = self.scalers_by_ticker[ticker]\n",
    "        \n",
    "        scaled_data_np = scaler.transform(original_data)\n",
    "        \n",
    "        scaled_df = pd.DataFrame(\n",
    "            scaled_data_np, \n",
    "            index=original_data.index, \n",
    "            columns=original_data.columns\n",
    "        )\n",
    "        return scaled_df\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        return self.tickers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cd5c9-b0a1-4c9a-9e94-d25c26ec639d",
   "metadata": {},
   "source": [
    "## Phase2-B : 윈도우 생성기 구현\n",
    "LLM에 넣기 위해 슬라이딩 윈도우 방식의 3D 데이터로 재구성합니다.\n",
    "<br>(샘플 수, 120일, 12개 채널) 형태의 numpy 배열(텐서)을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d4bd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Phase 2-A/C] DataHandler V2 초기화 ---\n",
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n",
      "\n",
      "--- [Phase 2-B] '010140' 윈도우 생성 테스트 ---\n",
      "\n",
      "[SUCCESS] 3D Tensors (Windows) created!\n",
      "  > X (Input Tensors) shape: (1259, 120, 12)\n",
      "  > y (Target Tensors) shape: (1259, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "# [2] --- 윈도우 생성기 함수 정의 ---\n",
    "# (Phase 2-B)\n",
    "\n",
    "def create_sliding_windows(data, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    DataFrame(2D)을 슬라이딩 윈도우(3D) numpy 배열로 변환합니다.\n",
    "    \"\"\"\n",
    "    data_np = data.values\n",
    "    n_samples = len(data_np)\n",
    "    X, y = [], []\n",
    "    \n",
    "    total_len = input_seq_len + output_seq_len\n",
    "    \n",
    "    for i in range(n_samples - total_len + 1):\n",
    "        input_window = data_np[i : i + input_seq_len]\n",
    "        output_window = data_np[i + input_seq_len : i + total_len]\n",
    "        X.append(input_window)\n",
    "        y.append(output_window)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- 3. Phase 2 파이프라인 실행 테스트 ---\n",
    "\n",
    "# (노트북 표준 세팅 코드에서 DATA_DIR이 정의되었다고 가정)\n",
    "# DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "print(\"--- [Phase 2-A/C] DataHandler V2 초기화 ---\")\n",
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "# 모델 하이퍼파라미터\n",
    "INPUT_SEQ_LEN = 120  # 120일 (Input)\n",
    "OUTPUT_SEQ_LEN = 10  # 10일 (Target)\n",
    "\n",
    "if '010140' in data_handler.get_all_tickers():\n",
    "    print(\"\\n--- [Phase 2-B] '010140' 윈도우 생성 테스트 ---\")\n",
    "    \n",
    "    # 1. 표준화된 2D 데이터 가져오기\n",
    "    scaled_df = data_handler.get_scaled_data_by_ticker('010140')\n",
    "    \n",
    "    # 2. 3D 윈도우 생성\n",
    "    X_train_samsung, y_train_samsung = create_sliding_windows(\n",
    "        scaled_df, \n",
    "        INPUT_SEQ_LEN, \n",
    "        OUTPUT_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[SUCCESS] 3D Tensors (Windows) created!\")\n",
    "    print(f\"  > X (Input Tensors) shape: {X_train_samsung.shape}\")\n",
    "    print(f\"  > y (Target Tensors) shape: {y_train_samsung.shape}\")\n",
    "else:\n",
    "    print(\"\\n[Error] '010140' Ticker not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev310-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
