{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aacd163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. ê²½ë¡œ ì„¤ì • ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b240ce5-2394-46d5-84b8-9b95150cd6cc",
   "metadata": {},
   "source": [
    "## Phase2-A : Data Handler ì •ì˜\n",
    "\n",
    "12ê°œ ì±„ë„ì˜ Scale ì¼ì¹˜ë¥¼ ìœ„í•´ Standardizationì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "<br>ëª¨ë“  12ê°œ ì±„ë„ì˜ Scaleì„í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì¸ í‘œì¤€ ì •ê·œ ë¶„í¬ë¡œ í†µì¼ì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a767163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# [1] --- DataHandler í´ë˜ìŠ¤ ì •ì˜ (V2: Scaler + zfill) ---\n",
    "# (Phase 2-Aì™€ 2-Cê°€ í•©ì³ì§„ ìµœì¢… ë²„ì „)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. ê²½ë¡œ ì„¤ì • ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    [V2] í‘œì¤€í™”(Standardization)ì™€ zfill(6)ì´ ì ìš©ëœ DataHandler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, train_end_date='2022-12-31'):\n",
    "        self.file_path = file_path\n",
    "        self.train_end_date = pd.to_datetime(train_end_date)\n",
    "        self.data_by_ticker = {}   # ì›ë³¸ ë°ì´í„°\n",
    "        self.scalers_by_ticker = {} # Tickerë³„ Scaler\n",
    "        self.tickers = []\n",
    "        \n",
    "        self._load_and_process_data()\n",
    "        self._fit_scalers()\n",
    "        \n",
    "    def _load_and_process_data(self):\n",
    "        try:\n",
    "            # 1. dtype=strë¡œ ì½ê¸°\n",
    "            df = pd.read_csv(\n",
    "                self.file_path, \n",
    "                parse_dates=['date'],\n",
    "                dtype={'ticker': str} \n",
    "            )\n",
    "            # 2. zfill(6)ë¡œ '0' ì±„ìš°ê¸°\n",
    "            df['ticker'] = df['ticker'].str.zfill(6)\n",
    "            df = df.set_index('date')\n",
    "            \n",
    "            self.tickers = df['ticker'].unique()\n",
    "            \n",
    "            for ticker in self.tickers:\n",
    "                ticker_df = df[df['ticker'] == ticker].copy()\n",
    "                channel_cols = [col for col in ticker_df.columns if col not in ['ticker']]\n",
    "                self.data_by_ticker[ticker] = ticker_df[channel_cols]\n",
    "            \n",
    "            print(f\"[DataHandler V2] Success: Loaded {len(self.tickers)} tickers.\")\n",
    "            print(f\"[DataHandler V2] Available tickers: {self.tickers}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DataHandler V2] Error loading data: {e}\")\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        [Data Leakage ë°©ì§€] í›ˆë ¨ ë°ì´í„°ë¡œë§Œ Scalerë¥¼ í•™ìŠµ(fit)\n",
    "        \"\"\"\n",
    "        print(f\"[DataHandler V2] Fitting scalers using data up to {self.train_end_date.date()}...\")\n",
    "        for ticker in self.tickers:\n",
    "            train_data = self.data_by_ticker[ticker].loc[:self.train_end_date]\n",
    "            if train_data.empty:\n",
    "                print(f\"  > Warning: No training data for {ticker}.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_data) # 'fit'ì€ í›ˆë ¨ ë°ì´í„°ë¡œë§Œ!\n",
    "            self.scalers_by_ticker[ticker] = scaler\n",
    "        print(\"[DataHandler V2] Scalers fitted.\")\n",
    "\n",
    "    def get_scaled_data_by_ticker(self, ticker):\n",
    "        \"\"\"\n",
    "        'transform'ì€ ì „ì²´ ë°ì´í„°ì— ì ìš©í•˜ì—¬ í‘œì¤€í™”ëœ DF ë°˜í™˜\n",
    "        \"\"\"\n",
    "        if ticker not in self.scalers_by_ticker:\n",
    "            print(f\"[DataHandler V2] Error: No scaler for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        original_data = self.data_by_ticker[ticker]\n",
    "        scaler = self.scalers_by_ticker[ticker]\n",
    "        \n",
    "        scaled_data_np = scaler.transform(original_data)\n",
    "        \n",
    "        scaled_df = pd.DataFrame(\n",
    "            scaled_data_np, \n",
    "            index=original_data.index, \n",
    "            columns=original_data.columns\n",
    "        )\n",
    "        return scaled_df\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        return self.tickers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cd5c9-b0a1-4c9a-9e94-d25c26ec639d",
   "metadata": {},
   "source": [
    "## Phase2-B : ìœˆë„ìš° ìƒì„±ê¸° êµ¬í˜„\n",
    "LLMì— ë„£ê¸° ìœ„í•´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ì˜ 3D ë°ì´í„°ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "<br>(ìƒ˜í”Œ ìˆ˜, 120ì¼, 12ê°œ ì±„ë„) í˜•íƒœì˜ numpy ë°°ì—´(í…ì„œ)ì„ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d4bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] --- ìœˆë„ìš° ìƒì„±ê¸° í•¨ìˆ˜ ì •ì˜ ---\n",
    "# (Phase 2-B)\n",
    "\n",
    "def create_sliding_windows(data, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    DataFrame(2D)ì„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(3D) numpy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    data_np = data.values\n",
    "    n_samples = len(data_np)\n",
    "    X, y = [], []\n",
    "    \n",
    "    total_len = input_seq_len + output_seq_len\n",
    "    \n",
    "    for i in range(n_samples - total_len + 1):\n",
    "        input_window = data_np[i : i + input_seq_len]\n",
    "        output_window = data_np[i + input_seq_len : i + total_len]\n",
    "        X.append(input_window)\n",
    "        y.append(output_window)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- 3. Phase 2 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---\n",
    "\n",
    "# (ë…¸íŠ¸ë¶ í‘œì¤€ ì„¸íŒ… ì½”ë“œì—ì„œ DATA_DIRì´ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
    "# DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "print(\"--- [Phase 2-A/C] DataHandler V2 ì´ˆê¸°í™” ---\")\n",
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "INPUT_SEQ_LEN = 120  # 120ì¼ (Input)\n",
    "OUTPUT_SEQ_LEN = 10  # 10ì¼ (Target)\n",
    "\n",
    "if '010140' in data_handler.get_all_tickers():\n",
    "    print(\"\\n--- [Phase 2-B] '010140' ìœˆë„ìš° ìƒì„± í…ŒìŠ¤íŠ¸ ---\")\n",
    "    \n",
    "    # 1. í‘œì¤€í™”ëœ 2D ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "    scaled_df = data_handler.get_scaled_data_by_ticker('010140')\n",
    "    \n",
    "    # 2. 3D ìœˆë„ìš° ìƒì„±\n",
    "    X_train_samsung, y_train_samsung = create_sliding_windows(\n",
    "        scaled_df, \n",
    "        INPUT_SEQ_LEN, \n",
    "        OUTPUT_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[SUCCESS] 3D Tensors (Windows) created!\")\n",
    "    print(f\"  > X (Input Tensors) shape: {X_train_samsung.shape}\")\n",
    "    print(f\"  > y (Target Tensors) shape: {y_train_samsung.shape}\")\n",
    "else:\n",
    "    print(\"\\n[Error] '010140' Ticker not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd16901-7773-4562-adf9-6da99c3483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. [Phase 2 ì‹¤í–‰] ë°ì´í„° ë¡œë“œ ë° ìœˆë„ìš° ìƒì„± ---\n",
    "# (ì»¤ë„ ì¬ì‹œì‘í–ˆìœ¼ë©´ ì´ ë¶€ë¶„ì„ ê¼­ ë‹¤ì‹œ ëŒë ¤ì•¼ ë³€ìˆ˜ê°€ ìƒê¹ë‹ˆë‹¤!)\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "print(\"--- [Phase 2] ë°ì´í„° ìƒì„± ë° ìœˆë„ìš° ë³€í™˜ ì‹œì‘ ---\")\n",
    "\n",
    "# 1. DataHandler ì´ˆê¸°í™”\n",
    "# (ìœ„ìª½ ì…€ì—ì„œ DataHandler í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)\n",
    "if 'DataHandler' not in locals():\n",
    "    print(\"âŒ [ERROR] DataHandler í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§¨ ìœ„ìª½ í´ë˜ìŠ¤ ì •ì˜ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”!\")\n",
    "else:\n",
    "    data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "    # 2. ìœˆë„ìš° ìƒì„±\n",
    "    INPUT_SEQ_LEN = 120\n",
    "    OUTPUT_SEQ_LEN = 10\n",
    "\n",
    "    if '010140' in data_handler.get_all_tickers():\n",
    "        scaled_df = data_handler.get_scaled_data_by_ticker('010140')\n",
    "        \n",
    "        # ì—¬ê¸°ì„œ X_train_samsung ë³€ìˆ˜ê°€ ìƒì„±ë©ë‹ˆë‹¤!\n",
    "        X_train_samsung, y_train_samsung = create_sliding_windows(\n",
    "            scaled_df, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN\n",
    "        )\n",
    "        print(f\"âœ… [Phase 2 ì™„ë£Œ] ìœˆë„ìš° ìƒì„±ë¨: {X_train_samsung.shape}\")\n",
    "    else:\n",
    "        print(\"âŒ [ERROR] 010140 ë°ì´í„° ì—†ìŒ\")\n",
    "\n",
    "# --- 2. [Phase 3-B] ë°ì´í„°ì…‹ ë° ë¡œë” ì •ì˜ ---\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# --- 3. [Phase 3-B] ì‹¤í–‰: Train/Val/Test ë¶„ë¦¬ ë° ë¡œë” ìƒì„± ---\n",
    "\n",
    "if 'X_train_samsung' in locals():\n",
    "    print(\"\\n--- [Phase 3-B] ë°ì´í„° ë¡œë” ìƒì„± ì‹œì‘ ---\")\n",
    "    \n",
    "    X_data = X_train_samsung\n",
    "    y_data = y_train_samsung\n",
    "    total_samples = len(X_data)\n",
    "    \n",
    "    # 7:1:2 ë¶„í• \n",
    "    train_size = int(total_samples * 0.7)\n",
    "    val_size = int(total_samples * 0.1)\n",
    "    \n",
    "    X_train = X_data[:train_size]\n",
    "    y_train = y_data[:train_size]\n",
    "    \n",
    "    X_val = X_data[train_size : train_size+val_size]\n",
    "    y_val = y_data[train_size : train_size+val_size]\n",
    "    \n",
    "    X_test = X_data[train_size+val_size:]\n",
    "    y_test = y_data[train_size+val_size:]\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset = ShipDataset(X_train, y_train)\n",
    "    val_dataset = ShipDataset(X_val, y_val)\n",
    "    test_dataset = ShipDataset(X_test, y_test)\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë” ìƒì„± (Batch Size 32)\n",
    "    BATCH_SIZE = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"âœ… [SUCCESS] ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(f\"  > Train Loader: {len(train_loader)} batches\")\n",
    "    print(f\"  > Val Loader  : {len(val_loader)} batches\")\n",
    "    print(f\"  > Test Loader : {len(test_loader)} batches\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ [ERROR] ë³€ìˆ˜ ìƒì„± ì‹¤íŒ¨. ìœ„ ì½”ë“œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7145971a-79c6-421e-9171-c4062824926a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‹¤ì‹œ)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "GPT2_PATH = os.path.join(PROJECT_ROOT, \"pretrained_models\", \"gpt2\")\n",
    "TIME_LLM_ROOT = os.path.join(PROJECT_ROOT, \"external\", \"time-llm\")\n",
    "\n",
    "if TIME_LLM_ROOT not in sys.path:\n",
    "    sys.path.append(TIME_LLM_ROOT)\n",
    "\n",
    "# 2. Time-LLM ì„í¬íŠ¸\n",
    "try:\n",
    "    import models.TimeLLM\n",
    "    import importlib\n",
    "    importlib.reload(models.TimeLLM) # ìˆ˜ì •ì‚¬í•­ ë°˜ì˜ì„ ìœ„í•´ ê°•ì œ ë¦¬ë¡œë“œ\n",
    "    from models.TimeLLM import Model as TimeLLM\n",
    "except ImportError:\n",
    "    print(\"âŒ Time-LLM import failed\")\n",
    "\n",
    "# 3. [ìµœì¢…] Configs í´ë˜ìŠ¤ (ë¹ ì§ì—†ì´ ë‹¤ ë„£ìŒ!)\n",
    "class Configs:\n",
    "    def __init__(self):\n",
    "        # [ê¸°ë³¸ ì„¤ì •]\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "        \n",
    "        # [ë°ì´í„° ì°¨ì›]\n",
    "        self.seq_len = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "        \n",
    "        # [LLM ì„¤ì •]\n",
    "        self.llm_model = 'GPT2' \n",
    "        self.llm_model_path = GPT2_PATH \n",
    "        self.llm_dim = 768       \n",
    "        self.llm_layers = 12     \n",
    "        \n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.d_model = 32\n",
    "        self.d_ff = 128\n",
    "        self.n_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        # [!!! ì—ëŸ¬ í•´ê²°ì‚¬: ëˆ„ë½ëœ ì„¤ì •ë“¤ ì¶”ê°€ !!!]\n",
    "        self.prompt_domain = 1   # <--- ì•„ê¹Œ ê·¸ ì—ëŸ¬ì˜ ì›ì¸!\n",
    "        self.content = \"Predict the future stock price based on financial indicators.\"\n",
    "        \n",
    "        # (í˜¹ì‹œ ëª¨ë¥¼ ë‹¤ìŒ ì—ëŸ¬ ë°©ì§€ìš© ì•ˆì „ì¥ì¹˜ë“¤)\n",
    "        self.embed = 'timeF'     # ì‹œê°„ ì„ë² ë”© ë°©ì‹\n",
    "        self.freq = 'd'          # ë°ì´í„° ë¹ˆë„ (Daily)\n",
    "        self.factor = 1          # Attention Factor\n",
    "        self.moving_avg = 25     # Moving Average Kernel size\n",
    "        self.e_layers = 2        # Encoder Layers\n",
    "        self.d_layers = 1        # Decoder Layers\n",
    "        self.top_k = 5           # Top-K optimization\n",
    "\n",
    "# 4. ì´ˆê¸°í™” ì‹¤í–‰\n",
    "if 'TimeLLM' in locals():\n",
    "    print(f\"â³ Initializing Time-LLM (Final Attempt)...\")\n",
    "    try:\n",
    "        configs = Configs()\n",
    "        model = TimeLLM(configs)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            \n",
    "        print(\"\\nğŸ‰ [SUCCESS] Real Time-LLM initialized! (ë“œë””ì–´ ì„±ê³µ!)\")\n",
    "        print(f\"  > LLM Backbone: GPT-2 (from {configs.llm_model_path})\")\n",
    "        print(f\"  > Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ [FAIL] Init Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16be1587-6d07-4cf4-bf34-f653b901c263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 111\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43mtrain_loader_global\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m    114\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader_global' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# â˜… GPU ì‚¬ìš© ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•¨)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import models.TimeLLM \n",
    "importlib.reload(models.TimeLLM) # ëª¨ë¸ êµ¬ì¡° ë¦¬ë¡œë“œ\n",
    "from models.TimeLLM import Model as TimeLLM\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# === ë¡œê·¸ ì €ì¥ ê²½ë¡œ ì„¤ì • ===\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_PATH = os.path.join(LOG_DIR, f\"train_log_{timestamp}.txt\")\n",
    "\n",
    "# === Logger í´ë˜ìŠ¤: console + file ë™ì‹œì— ì¶œë ¥ ===\n",
    "class Logger(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(file_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()  # ì¦‰ì‹œ ê¸°ë¡\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "# === sys.stdout êµì²´ ===\n",
    "sys.stdout = Logger(LOG_PATH)\n",
    "print(f\"[LOGGING] Training logs will be saved to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# --- 1. [Ultimate] Configs ì„¤ì • ---\n",
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seq_len = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "        self.llm_model = 'GPT2' \n",
    "        self.llm_model_path = GPT2_PATH \n",
    "        self.llm_dim = 768       \n",
    "        self.llm_layers = 6     \n",
    "        \n",
    "        # [ì „ëµ 2: í˜„ë¯¸ê²½ ëª¨ë“œ] \n",
    "        # 16ì¼ì”© ë³´ë˜ ê±¸ 8ì¼ë¡œ ì¤„ì—¬ì„œ ë” ë””í…Œì¼í•˜ê²Œ ë´…ë‹ˆë‹¤.\n",
    "        self.patch_len = 32      # (ê¸°ì¡´ 16 -> 8)\n",
    "        self.stride = 16          # (ê¸°ì¡´ 8 -> 4: ë” ì´˜ì´˜í•˜ê²Œ ê²¹ì¹¨)\n",
    "        \n",
    "        # [Monster Spec ìœ ì§€]\n",
    "        self.d_model = 256      \n",
    "        self.d_ff = 256          \n",
    "        self.n_heads = 12        \n",
    "        self.dropout = 0.02      \n",
    "        \n",
    "        self.prompt_domain = 1\n",
    "        \n",
    "        # [ì „ëµ 1: Rich PaP] \n",
    "        # ì¡°ì„ ì—… íŠ¹í™” ì •ë³´ë¥¼ ì•„ì£¼ êµ¬ì²´ì ìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
    "        self.content = (\n",
    "            \"Task: Forecast daily closing prices for Korean Shipbuilding companies. \"\n",
    "            \"Input Data: 12 channels including OHLC prices, Trading Volume, \"\n",
    "            \"and Macro-indicators: Brent Oil Price, USD/KRW Exchange Rate, Interest Rate, and BDI (Baltic Dry Index). \"\n",
    "            \"Context: Shipbuilding stocks are sensitive to Oil prices and BDI. \"\n",
    "            \"Analyze the 120-day trend, focusing on volatility and correlation between macro-indicators and stock price. \"\n",
    "            \"Predict the next 10 days.\"\n",
    "        )\n",
    "\n",
    "# 2. ëª¨ë¸ ì¬ì´ˆê¸°í™”\n",
    "configs = Configs()\n",
    "model = TimeLLM(configs)\n",
    "model.to(device).float()\n",
    "\n",
    "print(f\"ğŸ”¥ Start Ultimate Training on {device}...\")\n",
    "print(f\"   > Strategy: Rich Prompt + High Resolution (Patch 32)\")\n",
    "\n",
    "# 3. í•™ìŠµ ì„¤ì •\n",
    "# [ì „ëµ 3: ì§„ë“í•˜ê²Œ í•™ìŠµ]\n",
    "LEARNING_RATE = 0.00025  # (ê¸°ì¡´ 0.0005 -> 0.0001: ë” ì„¸ë°€í•˜ê²Œ)\n",
    "EPOCHS = 30             # (ê¸°ì¡´ 15 -> 30: ë” ì˜¤ë˜)\n",
    "ACCUM_STEPS = 8         # (Batch 4 * 8 = 32 íš¨ê³¼)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader_global, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, (batch_x, batch_y) in enumerate(progress_bar):\n",
    "        batch_x = batch_x.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        \n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]\n",
    "        \n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4).to(device).float()\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4).to(device).float()\n",
    "        dummy_dec_in = torch.zeros(B, Pred, C).to(device).float()\n",
    "        \n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "            \n",
    "        f_dim = -1 if configs.c_out == 1 else 0\n",
    "        preds = outputs[:, -configs.pred_len:, f_dim:]\n",
    "        true = batch_y\n",
    "        \n",
    "        loss = criterion(preds, true)\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        current_loss = loss.item() * ACCUM_STEPS\n",
    "        total_loss += current_loss\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix({'loss': f\"{current_loss:.5f}\", 'lr': f\"{current_lr:.6f}\"})\n",
    "        \n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader_global)\n",
    "    print(f\"Epoch {epoch+1} Done. Avg Loss: {avg_loss:.5f}\")\n",
    "\n",
    "# 5. ìµœì¢… ì €ì¥ (ë®ì–´ì“°ê¸°)\n",
    "SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, \"ship_time_llm_tmp2.pth\")\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\nğŸ’¾ [ULTIMATE] Model Saved: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78447a5c-eed9-4f4a-8f6f-b69d60a0a3ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Model moved to CPU for visualization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ë°ì´í„° ë¡œë”ì—ì„œ í•˜ë‚˜ ë½‘ê¸°\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader_global\u001b[49m)\n\u001b[1;32m     16\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ë°ì´í„°ë„ CPUì— ë‘¡ë‹ˆë‹¤ (êµ³ì´ GPUë¡œ ë³´ë‚¼ í•„ìš” ì—†ìŒ)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader_global' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. ê¸´ê¸‰! ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ CPUë¡œ í”¼ì‹ ì‹œí‚µë‹ˆë‹¤.\n",
    "# ==========================================\n",
    "# GPU ë©”ëª¨ë¦¬ê°€ ê½‰ ì°¼ìœ¼ë‹ˆ CPUë¡œ ëª¨ë¸ì„ ì˜®ê¹ë‹ˆë‹¤.\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model moved to CPU for visualization.\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë”ì—ì„œ í•˜ë‚˜ ë½‘ê¸°\n",
    "data_iter = iter(train_loader_global)\n",
    "batch_x, batch_y = next(data_iter)\n",
    "\n",
    "# ë°ì´í„°ë„ CPUì— ë‘¡ë‹ˆë‹¤ (êµ³ì´ GPUë¡œ ë³´ë‚¼ í•„ìš” ì—†ìŒ)\n",
    "batch_x = batch_x.cpu().float()\n",
    "batch_y = batch_y.cpu().float()\n",
    "\n",
    "# ==========================================\n",
    "# 2. CPUì—ì„œ ì¶”ë¡  (Inference)\n",
    "# ==========================================\n",
    "print(\"ğŸ¤– CPUì—ì„œ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘... (ë©”ëª¨ë¦¬ ê±±ì • NO)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ê°€ì§œ ì…ë ¥ ìƒì„± (CPUìš©)\n",
    "    B, Seq, C = batch_x.shape\n",
    "    Pred = batch_y.shape[1]\n",
    "    \n",
    "    dummy_mark_enc = torch.zeros(B, Seq, 4).cpu().float()\n",
    "    dummy_mark_dec = torch.zeros(B, Pred, 4).cpu().float()\n",
    "    dummy_dec_in = torch.zeros(B, Pred, C).cpu().float()\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "    \n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "# ==========================================\n",
    "# 3. ì‹œê°í™”\n",
    "# ==========================================\n",
    "sample_idx = 0\n",
    "pred_seq = outputs[sample_idx, :, 0].numpy() # ì´ë¯¸ CPUë¼ .cpu() ë¶ˆí•„ìš”\n",
    "true_seq = batch_y[sample_idx, :, 0].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(Pred), true_seq, label='Actual (Ground Truth)', \n",
    "         marker='o', color='black', linewidth=2, markersize=8)\n",
    "plt.plot(range(Pred), pred_seq, label='AI Prediction (Forecast)', \n",
    "         marker='x', linestyle='--', color='red', linewidth=2, markersize=8)\n",
    "\n",
    "plt.title(f\"Ultimate Time-LLM Forecast (Loss: 0.4224) - Sample #{sample_idx}\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Future Days (Day 1 to 10)\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0351ea0-6b5f-40d1-ae59-2a38a63b5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Using Device: cuda (RTX 5090 Power!)\n",
      "ğŸ“‚ Loading model from: /workspace/ship-ai/models/ship_time_llm_tmp2.pth\n",
      "âœ… Model successfully loaded on GPU!\n",
      "\n",
      "ğŸ“Š Calculating Directional Accuracy (DA) on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [01:00<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Final DA (GPU Verified): 55.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import models.TimeLLM \n",
    "\n",
    "# 1. ê¸°ë³¸ ì„¤ì •\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "GPT2_PATH = os.path.join(PROJECT_ROOT, \"pretrained_models\", \"gpt2\")\n",
    "SAVE_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ship_time_llm_tmp2.pth\")\n",
    "\n",
    "# [GPU ì„¤ì •] \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”¥ Using Device: {device} (RTX 5090 Power!)\")\n",
    "\n",
    "# 2. Configs (Ultimate ì„¤ì • ë™ì¼)\n",
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 0\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seq_len = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "        self.llm_model = 'GPT2' \n",
    "        self.llm_model_path = GPT2_PATH \n",
    "        self.llm_dim = 768       \n",
    "        self.llm_layers = 12     \n",
    "        self.patch_len = 8   \n",
    "        self.stride = 4      \n",
    "        self.d_model = 768   \n",
    "        self.d_ff = 768      \n",
    "        self.n_heads = 12        \n",
    "        self.dropout = 0.0\n",
    "        self.prompt_domain = 1\n",
    "        self.content = \"Predict...\"\n",
    "\n",
    "# 3. ëª¨ë¸ êµ¬ì¡° ìƒì„±\n",
    "importlib.reload(models.TimeLLM)\n",
    "from models.TimeLLM import Model as TimeLLM\n",
    "\n",
    "configs = Configs()\n",
    "model = TimeLLM(configs)\n",
    "\n",
    "# 4. [í•µì‹¬] GPUë¡œ ëª¨ë¸ ë¡œë“œ\n",
    "if os.path.exists(SAVE_PATH):\n",
    "    print(f\"ğŸ“‚ Loading model from: {SAVE_PATH}\")\n",
    "    \n",
    "    # (1) GPU ë©”ëª¨ë¦¬ë¡œ ì§ì ‘ ë¡œë“œ\n",
    "    state_dict = torch.load(SAVE_PATH, map_location=device) \n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # (2) ëª¨ë¸ì„ GPUì— ì˜¬ë¦¼\n",
    "    model.to(device).float()\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"âœ… Model successfully loaded on GPU!\")\n",
    "else:\n",
    "    print(\"âŒ Model file not found.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ==========================================================\n",
    "# 5. ë°©í–¥ì„± ì •í™•ë„(DA) ê³„ì‚° (GPU ê°€ì†)\n",
    "# ==========================================================\n",
    "if 'train_loader_global' not in locals():\n",
    "    print(\"âŒ 'train_loader_global'ì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ìª½ ë°ì´í„° ë¡œë” ë³µêµ¬ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"\\nğŸ“Š Calculating Directional Accuracy (DA) on GPU...\")\n",
    "    correct_direction = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(train_loader_global):\n",
    "            # [ìˆ˜ì •] ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "            \n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "            \n",
    "            # [ìˆ˜ì •] ê°€ì§œ ì…ë ¥ë„ GPUë¡œ ìƒì„±\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4).to(device).float()\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4).to(device).float()\n",
    "            dummy_dec_in = torch.zeros(B, Pred, C).to(device).float()\n",
    "            \n",
    "            # ì¶”ë¡ \n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "            \n",
    "            # ê²°ê³¼ ê³„ì‚° (GPU í…ì„œë¥¼ CPUë¡œ ë‚´ë ¤ì„œ numpy ë³€í™˜)\n",
    "            current = batch_x[:, -1, 0].cpu().numpy()\n",
    "            true_fut = batch_y[:, -1, 0].cpu().numpy()\n",
    "            pred_fut = outputs[:, -1, 0].cpu().numpy()\n",
    "            \n",
    "            hits = ((true_fut - current) * (pred_fut - current)) > 0\n",
    "            correct_direction += np.sum(hits)\n",
    "            total_samples += B\n",
    "\n",
    "    accuracy = (correct_direction / total_samples) * 100\n",
    "    print(f\"\\nğŸ¯ Final DA (GPU Verified): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bebf33a-3979-41c1-9d0a-cd95c4dd038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š High Confidence ìŠ¹ë¥  ë¶„ì„ ì¤‘...\n",
      "----------------------------------------\n",
      "ğŸ¯ [í•„í„°ë§ ì ìš© í›„] ìŠ¹ë¥  ë¶„ì„ (ê¸°ì¤€: ë³€ë™í­ 2.0%)\n",
      "   > ì§„ì… íšŸìˆ˜: 4144íšŒ (ì „ì²´ì˜ ì•½ 90.7%)\n",
      "   > ìŠ¹ë¦¬ íšŸìˆ˜: 2302íšŒ\n",
      "   > í•„í„°ë§ ìŠ¹ë¥ : 55.55% ğŸ”¥\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¬ [ì‹¬í™” ê²€ì¦] í™•ì‹  í•„í„°ë§ (Confidence Filtering) ìŠ¹ë¥  ê³„ì‚°\n",
    "\n",
    "print(\"ğŸ“Š High Confidence ìŠ¹ë¥  ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "total_trades = 0\n",
    "winning_trades = 0\n",
    "threshold = 0.02  # \"2% ì´ìƒ ì˜¤ë¥´ê±°ë‚˜ ë‚´ë¦°ë‹¤ê³  í•  ë•Œë§Œ ì§„ì…í•˜ê² ë‹¤\" (ê¸°ì¤€ì )\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in train_loader_global:\n",
    "        batch_x = batch_x.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        \n",
    "        # ... (ì¶”ë¡  ì½”ë“œ ë™ì¼) ...\n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]\n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4).to(device).float()\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4).to(device).float()\n",
    "        dummy_dec_in = torch.zeros(B, Pred, C).to(device).float()\n",
    "        \n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "        \n",
    "        # ê°’ ì¶”ì¶œ\n",
    "        current = batch_x[:, -1, 0].cpu().numpy()\n",
    "        true_fut = batch_y[:, -1, 0].cpu().numpy()\n",
    "        pred_fut = outputs[:, -1, 0].cpu().numpy()\n",
    "        \n",
    "        # [í•µì‹¬ ë¡œì§] ì˜ˆì¸¡ ë³€ë™í­ ê³„ì‚°\n",
    "        pred_change_pct = (pred_fut - current) / current\n",
    "        \n",
    "        # 2% ì´ìƒ ê¸‰ë“±/ê¸‰ë½ ì˜ˆì¸¡í•œ ê²½ìš°ë§Œ ì¹´ìš´íŠ¸ (í•„í„°ë§)\n",
    "        mask = np.abs(pred_change_pct) >= threshold \n",
    "        \n",
    "        if np.sum(mask) > 0:\n",
    "            true_chg = true_fut[mask] - current[mask]\n",
    "            pred_chg = pred_fut[mask] - current[mask]\n",
    "            \n",
    "            # ë°©í–¥ ë§ì¶¤?\n",
    "            hits = (true_chg * pred_chg) > 0\n",
    "            \n",
    "            winning_trades += np.sum(hits)\n",
    "            total_trades += np.sum(mask)\n",
    "\n",
    "if total_trades > 0:\n",
    "    win_rate = (winning_trades / total_trades) * 100\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"ğŸ¯ [í•„í„°ë§ ì ìš© í›„] ìŠ¹ë¥  ë¶„ì„ (ê¸°ì¤€: ë³€ë™í­ {threshold*100}%)\")\n",
    "    print(f\"   > ì§„ì… íšŸìˆ˜: {total_trades}íšŒ (ì „ì²´ì˜ ì•½ {total_trades/total_samples*100:.1f}%)\")\n",
    "    print(f\"   > ìŠ¹ë¦¬ íšŸìˆ˜: {winning_trades}íšŒ\")\n",
    "    print(f\"   > í•„í„°ë§ ìŠ¹ë¥ : {win_rate:.2f}% ğŸ”¥\")\n",
    "    print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"âš ï¸ í•´ë‹¹ ê¸°ì¤€ë§Œí¼ ê°•í•˜ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b7c874-054c-4c69-850d-6556c854b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š [Sniper Test] ê¸°ì¤€ë³„ ìŠ¹ë¥  ë° ìˆ˜ìµì„± ë¶„ì„\n",
      "=================================================================\n",
      "ê¸°ì¤€(Threshold)   | ì§„ì…(%)        | ìŠ¹ë¥ (Win%)     | í‰ê· ìˆ˜ìµ(Avg%)     \n",
      "-----------------------------------------------------------------\n",
      "   3% ì´ìƒ ë³€ë™ |  86.9% (3973) |  55.83% ğŸ”¥   | +1122.66% ğŸ’°\n",
      "   5% ì´ìƒ ë³€ë™ |  78.4% (3585) |  56.15% ğŸ”¥   | +1244.16% ğŸ’°\n",
      "   7% ì´ìƒ ë³€ë™ |  70.1% (3202) |  56.90% ğŸ”¥   | +1393.22% ğŸ’°\n",
      "  10% ì´ìƒ ë³€ë™ |  59.9% (2740) |  57.85% ğŸ”¥   | +1627.50% ğŸ’°\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ë¹ ë¥¸ ê³„ì‚°)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "thresholds = [0.03, 0.05, 0.07, 0.10] # 3%, 5%, 7%, 10% ê¸°ì¤€ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "print(\"ğŸ“Š [Sniper Test] ê¸°ì¤€ë³„ ìŠ¹ë¥  ë° ìˆ˜ìµì„± ë¶„ì„\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'ê¸°ì¤€(Threshold)':<15} | {'ì§„ì…(%)':<12} | {'ìŠ¹ë¥ (Win%)':<12} | {'í‰ê· ìˆ˜ìµ(Avg%)':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ì „ì²´ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ (ì†ë„ ìµœì í™”)\n",
    "    all_current = []\n",
    "    all_true_fut = []\n",
    "    all_pred_fut = []\n",
    "    \n",
    "    for batch_x, batch_y in train_loader_global:\n",
    "        batch_x = batch_x.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        \n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]\n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4).to(device).float()\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4).to(device).float()\n",
    "        dummy_dec_in = torch.zeros(B, Pred, C).to(device).float()\n",
    "        \n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "        \n",
    "        # CPUë¡œ ì´ë™ í›„ ì €ì¥\n",
    "        all_current.append(batch_x[:, -1, 0].cpu().numpy())\n",
    "        all_true_fut.append(batch_y[:, -1, 0].cpu().numpy())\n",
    "        all_pred_fut.append(outputs[:, -1, 0].cpu().numpy())\n",
    "\n",
    "    # Numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    all_current = np.concatenate(all_current)\n",
    "    all_true_fut = np.concatenate(all_true_fut)\n",
    "    all_pred_fut = np.concatenate(all_pred_fut)\n",
    "    \n",
    "    # --- ê¸°ì¤€ë³„ í…ŒìŠ¤íŠ¸ ë£¨í”„ ---\n",
    "    for th in thresholds:\n",
    "        # ì˜ˆì¸¡ ë³€ë™ë¥  ê³„ì‚°\n",
    "        pred_change_pct = (all_pred_fut - all_current) / all_current\n",
    "        \n",
    "        # í•„í„°ë§ (ì ˆëŒ€ê°’ ê¸°ì¤€)\n",
    "        mask = np.abs(pred_change_pct) >= th\n",
    "        \n",
    "        total_trades = np.sum(mask)\n",
    "        \n",
    "        if total_trades > 0:\n",
    "            # ì‹¤ì œ ë³€ë™ë¥ \n",
    "            true_change_pct = (all_true_fut[mask] - all_current[mask]) / all_current[mask]\n",
    "            # ì˜ˆì¸¡ ë³€ë™ë¥  (í•„í„°ë§ ëœ ê²ƒë§Œ)\n",
    "            pred_change_pct_filtered = pred_change_pct[mask]\n",
    "            \n",
    "            # 1. ìŠ¹ë¥  (ë°©í–¥ ì¼ì¹˜)\n",
    "            hits = (true_change_pct * pred_change_pct_filtered) > 0\n",
    "            win_rate = np.mean(hits) * 100\n",
    "            \n",
    "            # 2. ìˆ˜ìµì„± (ë°©í–¥ ë§ìœ¼ë©´ ìˆ˜ìµ, í‹€ë¦¬ë©´ ì†ì‹¤)\n",
    "            # (Long/Short ì „ëµ ê°€ì •: ì˜¤ë¥¼ ê±°ë¼ ì˜ˆì¸¡í•˜ë©´ ë§¤ìˆ˜, ë‚´ë¦´ ê±°ë¼ ì˜ˆì¸¡í•˜ë©´ ë§¤ë„)\n",
    "            # ìˆ˜ìµ = ì˜ˆì¸¡ë°©í–¥ * ì‹¤ì œë³€ë™\n",
    "            # ì˜ˆ: ì˜ˆì¸¡(+), ì‹¤ì œ(+) -> ìˆ˜ìµ / ì˜ˆì¸¡(+), ì‹¤ì œ(-) -> ì†ì‹¤\n",
    "            returns = np.sign(pred_change_pct_filtered) * true_change_pct\n",
    "            avg_return = np.mean(returns) * 100\n",
    "            \n",
    "            participation = (total_trades / len(all_current)) * 100\n",
    "            \n",
    "            print(f\"{th*100:>4.0f}% ì´ìƒ ë³€ë™ | {participation:>5.1f}% ({total_trades}) | {win_rate:>6.2f}% ğŸ”¥   | {avg_return:>+6.2f}% ğŸ’°\")\n",
    "        else:\n",
    "            print(f\"{th*100:>4.0f}% ì´ìƒ ë³€ë™ |   0.0% (0)    |    -         |    -\")\n",
    "\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e254fad7-cb13-4b8e-89f2-ddcf081709a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ ì‹¤ì œ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[43meval_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m eval_loader(val_loader,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36meval_loader\u001b[0;34m(loader, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m dummy_mark_dec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, Pred, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m dummy_dec_in   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, Pred, C)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_dec_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1783\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1794\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1796\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/ship-ai/external/time-llm/models/TimeLLM.py:203\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/ship-ai/external/time-llm/models/TimeLLM.py:241\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    237\u001b[0m     prompt\u001b[38;5;241m.\u001b[39mappend(prompt_)\n\u001b[1;32m    239\u001b[0m x_enc \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39mreshape(B, N, T)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 241\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    242\u001b[0m prompt_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(prompt\u001b[38;5;241m.\u001b[39mto(x_enc\u001b[38;5;241m.\u001b[39mdevice))  \u001b[38;5;66;03m# (batch, prompt_token, dim)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m source_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2938\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2938\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3026\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3023\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3024\u001b[0m         )\n\u001b[1;32m   3025\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3028\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3049\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3050\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3069\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3227\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3218\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3219\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3220\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3225\u001b[0m )\n\u001b[0;32m-> 3227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3229\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils.py:893\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 893\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils.py:945\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    943\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m first_ids, second_ids \u001b[38;5;129;01min\u001b[39;00m batch_ids_pairs:\n\u001b[0;32m--> 945\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecond_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPaddingStrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDO_NOT_PAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we pad in batch afterward\u001b[39;49;00m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We convert the whole batch to tensors at the end\u001b[39;49;00m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_special_tokens:\n\u001b[1;32m   3602\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inputs_with_special_tokens(ids, pair_ids)\n\u001b[0;32m-> 3603\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_token_type_ids_from_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3605\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m ids \u001b[38;5;241m+\u001b[39m pair_ids \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;28;01melse\u001b[39;00m ids\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3475\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.create_token_type_ids_from_sequences\u001b[0;34m(self, token_ids_0, token_ids_1)\u001b[0m\n\u001b[1;32m   3459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_token_type_ids_from_sequences\u001b[39m(\n\u001b[1;32m   3460\u001b[0m     \u001b[38;5;28mself\u001b[39m, token_ids_0: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], token_ids_1: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3463\u001b[0m \u001b[38;5;124;03m    Create the token type IDs corresponding to the sequences passed. [What are token type\u001b[39;00m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;124;03m    IDs?](../glossary#token-type-ids)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;124;03m        `list[int]`: The token type ids.\u001b[39;00m\n\u001b[1;32m   3474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3475\u001b[0m     cls_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3476\u001b[0m     sep_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msep_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_ids_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1105\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1106\u001b[0m     key_without_id \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m   1107\u001b[0m     key_is_special_id \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "def eval_loader(loader, name=\"train\"):\n",
    "    model.eval()\n",
    "\n",
    "    mse_model_list = []\n",
    "    mse_naive_list = []\n",
    "    dir_acc_model_list = []\n",
    "    dir_acc_naive_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()      # (B, Seq, C)\n",
    "            batch_y = batch_y.to(device).float()      # (B, Pred, C)\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4).to(device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4).to(device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C).to(device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # --- ëª¨ë¸ ì˜ˆì¸¡ (0ë²ˆ ì±„ë„ ê¸°ì¤€: ì¢…ê°€ë¼ê³  ê°€ì •) ---\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]    # (B, Pred, C)\n",
    "\n",
    "            true = batch_y[:, :, 0]    # (B, Pred)\n",
    "            pred = preds[:, :, 0]      # (B, Pred)\n",
    "\n",
    "            # ===== 1) MSE ë¹„êµ =====\n",
    "            mse_model = torch.mean((pred - true) ** 2).item()\n",
    "\n",
    "            # naive: ë§ˆì§€ë§‰ ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ 10ì¼ ë³µì‚¬ (ë ˆë²¨ ê¸°ì¤€ baseline)\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)  # (B, Pred)\n",
    "            mse_naive = torch.mean((naive - true) ** 2).item()\n",
    "\n",
    "            mse_model_list.append(mse_model)\n",
    "            mse_naive_list.append(mse_naive)\n",
    "\n",
    "            # ===== 2) ë°©í–¥ ì •í™•ë„(DIR%) ë¹„êµ =====\n",
    "            true_ret = true[:, 1:] - true[:, :-1]     # (B, Pred-1)\n",
    "            pred_ret = pred[:, 1:] - pred[:, :-1]\n",
    "\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            # naive ë°©í–¥: ë§ˆì§€ë§‰ ì…ë ¥ êµ¬ê°„ì˜ ë°©í–¥ì„ ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]   # (B,)\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            # true ë³€í™”ê°€ 0ì¸ êµ¬ê°„ì€ ì œì™¸\n",
    "            mask = true_sign != 0\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            model_correct = (pred_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "            naive_correct = (naive_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "\n",
    "            dir_acc_model_list.append(model_correct)\n",
    "            dir_acc_naive_list.append(naive_correct)\n",
    "            \n",
    "\n",
    "    avg_mse_model = np.mean(mse_model_list)\n",
    "    avg_mse_naive = np.mean(mse_naive_list)\n",
    "    avg_dir_model = np.mean(dir_acc_model_list) if len(dir_acc_model_list) > 0 else float('nan')\n",
    "    avg_dir_naive = np.mean(dir_acc_naive_list) if len(dir_acc_naive_list) > 0 else float('nan')\n",
    "\n",
    "    print(f\"[{name}] MSE   model={avg_mse_model:.4f}, naive={avg_mse_naive:.4f}\")\n",
    "    print(f\"[{name}] DIR%  model={avg_dir_model*100:.2f}%, naive={avg_dir_naive*100:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ğŸ”¹ ì‹¤ì œ ì‹¤í–‰\n",
    "eval_loader(train_loader, \"train\")\n",
    "eval_loader(val_loader,   \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5c729a-cd50-40bd-b20f-f77a2044db3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_last_scaled: 0.9607056379318237\n",
      "Y_mean_scaled: 0.7976146936416626\n",
      "Pred_mean_scaled: 3.3096923828125\n"
     ]
    }
   ],
   "source": [
    "# 1) ì…ë ¥ ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ê°’ (scaled)\n",
    "print(\"X_last_scaled:\", batch_x[0, -1, 0].item())\n",
    "\n",
    "# 2) ì •ë‹µ ì‹œí€€ìŠ¤ í‰ê· ê°’ (scaled)\n",
    "print(\"Y_mean_scaled:\", batch_y[0, :, 0].mean().item())\n",
    "\n",
    "# 3) ì˜ˆì¸¡ê°’ í‰ê·  (scaled)\n",
    "print(\"Pred_mean_scaled:\", pred[0].mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca332946-b7a9-44aa-8798-216cc41ed5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18678079545497894\n"
     ]
    }
   ],
   "source": [
    "true_ret = true[:, -1] - true[:, 0]\n",
    "pred_ret = pred[:, -1] - pred[:, 0]\n",
    "\n",
    "ret_mse = torch.mean((true_ret - pred_ret)**2).item()\n",
    "print(ret_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91b73cd0-bd67-4fca-ae5f-6a8dc2007ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [val] Horizonë³„ MSE (h=0ì€ +1ì¼ì°¨) ===\n",
      "h+1: MSE_model=0.0547, MSE_naive=0.0247\n",
      "h+2: MSE_model=0.0824, MSE_naive=0.0538\n",
      "h+3: MSE_model=0.1192, MSE_naive=0.0839\n",
      "h+4: MSE_model=0.1401, MSE_naive=0.1136\n",
      "h+5: MSE_model=0.1680, MSE_naive=0.1353\n",
      "h+6: MSE_model=0.2096, MSE_naive=0.1596\n",
      "h+7: MSE_model=0.2344, MSE_naive=0.1839\n",
      "h+8: MSE_model=0.2915, MSE_naive=0.2140\n",
      "h+9: MSE_model=0.3225, MSE_naive=0.2456\n",
      "h+10: MSE_model=0.3631, MSE_naive=0.2776\n",
      "\n",
      "=== [val] Horizonë³„ ë°©í–¥ ì •í™•ë„ (1~9ì¼ êµ¬ê°„) ===\n",
      "êµ¬ê°„ 1->2: DIR_model=42.60%, DIR_naive=55.70%\n",
      "êµ¬ê°„ 2->3: DIR_model=55.90%, DIR_naive=41.68%\n",
      "êµ¬ê°„ 3->4: DIR_model=56.67%, DIR_naive=53.23%\n",
      "êµ¬ê°„ 4->5: DIR_model=54.55%, DIR_naive=43.29%\n",
      "êµ¬ê°„ 5->6: DIR_model=52.70%, DIR_naive=44.29%\n",
      "êµ¬ê°„ 6->7: DIR_model=54.79%, DIR_naive=45.02%\n",
      "êµ¬ê°„ 7->8: DIR_model=51.03%, DIR_naive=50.14%\n",
      "êµ¬ê°„ 8->9: DIR_model=60.23%, DIR_naive=49.92%\n",
      "êµ¬ê°„ 9->10: DIR_model=49.06%, DIR_naive=45.70%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.float64(0.05470380745828152),\n",
       "  np.float64(0.08240102417767048),\n",
       "  np.float64(0.11920730024576187),\n",
       "  np.float64(0.14014775305986404),\n",
       "  np.float64(0.168045561760664),\n",
       "  np.float64(0.20955790579319),\n",
       "  np.float64(0.23438460379838943),\n",
       "  np.float64(0.29151125252246857),\n",
       "  np.float64(0.3225190341472626),\n",
       "  np.float64(0.3630925267934799)],\n",
       " [np.float64(0.024657076224684715),\n",
       "  np.float64(0.05379923526197672),\n",
       "  np.float64(0.08392051979899406),\n",
       "  np.float64(0.11361845582723618),\n",
       "  np.float64(0.13533006608486176),\n",
       "  np.float64(0.15955558605492115),\n",
       "  np.float64(0.1839192993938923),\n",
       "  np.float64(0.21397481113672256),\n",
       "  np.float64(0.2455967739224434),\n",
       "  np.float64(0.27761025726795197)],\n",
       " [np.float64(42.59877875447273),\n",
       "  np.float64(55.89619427919388),\n",
       "  np.float64(56.67205452919006),\n",
       "  np.float64(54.54741418361664),\n",
       "  np.float64(52.70114988088608),\n",
       "  np.float64(54.79166731238365),\n",
       "  np.float64(51.02909505367279),\n",
       "  np.float64(60.22988557815552),\n",
       "  np.float64(49.06250089406967)],\n",
       " [np.float64(55.700431764125824),\n",
       "  np.float64(41.684626787900925),\n",
       "  np.float64(53.22916731238365),\n",
       "  np.float64(43.293822556734085),\n",
       "  np.float64(44.2941814661026),\n",
       "  np.float64(45.01795992255211),\n",
       "  np.float64(50.14367774128914),\n",
       "  np.float64(49.91918131709099),\n",
       "  np.float64(45.69504335522652)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def eval_horizon(loader, name=\"val\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    Pred = configs.pred_len\n",
    "\n",
    "    mse_model_h = [[] for _ in range(Pred)]\n",
    "    mse_naive_h = [[] for _ in range(Pred)]\n",
    "    dir_model_h = [[] for _ in range(Pred-1)]\n",
    "    dir_naive_h = [[] for _ in range(Pred-1)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()   # (B, Seq, C)\n",
    "            batch_y = batch_y.to(device).float()   # (B, Pred, C)\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]   # (B, Pred, C)\n",
    "\n",
    "            true = batch_y[:, :, 0]   # (B, Pred)\n",
    "            pred = preds[:, :, 0]     # (B, Pred)\n",
    "\n",
    "            # naive: ë§ˆì§€ë§‰ ì…ë ¥ê°’ ê³ ì •\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)  # (B, Pred)\n",
    "\n",
    "            # --- 1) Horizonë³„ MSE ---\n",
    "            for h in range(Pred):\n",
    "                mse_m = torch.mean((pred[:, h] - true[:, h])**2).item()\n",
    "                mse_n = torch.mean((naive[:, h] - true[:, h])**2).item()\n",
    "                mse_model_h[h].append(mse_m)\n",
    "                mse_naive_h[h].append(mse_n)\n",
    "\n",
    "            # --- 2) Horizonë³„ ë°©í–¥ ì •í™•ë„ ---\n",
    "            true_ret  = true[:, 1:] - true[:, :-1]   # (B, Pred-1)\n",
    "            pred_ret  = pred[:, 1:] - pred[:, :-1]\n",
    "            naive_ret = naive[:, 1:] - naive[:, :-1]\n",
    "\n",
    "            true_sign  = torch.sign(true_ret)\n",
    "            pred_sign  = torch.sign(pred_ret)\n",
    "\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            for h in range(Pred-1):\n",
    "                ts = true_sign[:, h]\n",
    "                ps = pred_sign[:, h]\n",
    "                ns = naive_sign[:, h]\n",
    "\n",
    "                mask = ts != 0\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                acc_m = (ps[mask] == ts[mask]).float().mean().item()\n",
    "                acc_n = (ns[mask] == ts[mask]).float().mean().item()\n",
    "\n",
    "                dir_model_h[h].append(acc_m)\n",
    "                dir_naive_h[h].append(acc_n)\n",
    "\n",
    "    # í‰ê·  ì·¨í•˜ê¸°\n",
    "    mse_model_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_model_h]\n",
    "    mse_naive_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_naive_h]\n",
    "    dir_model_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_model_h]\n",
    "    dir_naive_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_naive_h]\n",
    "\n",
    "    print(f\"=== [{name}] Horizonë³„ MSE (h=0ì€ +1ì¼ì°¨) ===\")\n",
    "    for h in range(Pred):\n",
    "        print(f\"h+{h+1}: MSE_model={mse_model_h[h]:.4f}, MSE_naive={mse_naive_h[h]:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== [{name}] Horizonë³„ ë°©í–¥ ì •í™•ë„ (1~9ì¼ êµ¬ê°„) ===\")\n",
    "    for h in range(Pred-1):\n",
    "        print(f\"êµ¬ê°„ {h+1}->{h+2}: DIR_model={dir_model_h[h]:.2f}%, DIR_naive={dir_naive_h[h]:.2f}%\")\n",
    "\n",
    "    return mse_model_h, mse_naive_h, dir_model_h, dir_naive_h\n",
    "\n",
    "\n",
    "# ì‹¤ì œ ì‹¤í–‰ (val ê¸°ì¤€)\n",
    "eval_horizon(val_loader, \"val\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
