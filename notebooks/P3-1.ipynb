{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc247867-0e18-4946-9088-7f31ecabb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# [1] --- DataHandler ÌÅ¥ÎûòÏä§ Ï†ïÏùò (V2: Scaler + zfill) ---\n",
    "# (Phase 2-AÏôÄ 2-CÍ∞Ä Ìï©Ï≥êÏßÑ ÏµúÏ¢Ö Î≤ÑÏ†Ñ)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. Í≤ΩÎ°ú ÏÑ§Ï†ï ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    [V2] ÌëúÏ§ÄÌôî(Standardization)ÏôÄ zfill(6)Ïù¥ Ï†ÅÏö©Îêú DataHandler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, train_end_date='2022-12-31'):\n",
    "        self.file_path = file_path\n",
    "        self.train_end_date = pd.to_datetime(train_end_date)\n",
    "        self.data_by_ticker = {}   # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞\n",
    "        self.scalers_by_ticker = {} # TickerÎ≥Ñ Scaler\n",
    "        self.tickers = []\n",
    "        \n",
    "        self._load_and_process_data()\n",
    "        self._fit_scalers()\n",
    "        \n",
    "    def _load_and_process_data(self):\n",
    "        try:\n",
    "            # 1. dtype=strÎ°ú ÏùΩÍ∏∞\n",
    "            df = pd.read_csv(\n",
    "                self.file_path, \n",
    "                parse_dates=['date'],\n",
    "                dtype={'ticker': str} \n",
    "            )\n",
    "            # 2. zfill(6)Î°ú '0' Ï±ÑÏö∞Í∏∞\n",
    "            df['ticker'] = df['ticker'].str.zfill(6)\n",
    "            df = df.set_index('date')\n",
    "            \n",
    "            self.tickers = df['ticker'].unique()\n",
    "            \n",
    "            for ticker in self.tickers:\n",
    "                ticker_df = df[df['ticker'] == ticker].copy()\n",
    "                channel_cols = [col for col in ticker_df.columns if col not in ['ticker']]\n",
    "                self.data_by_ticker[ticker] = ticker_df[channel_cols]\n",
    "            \n",
    "            print(f\"[DataHandler V2] Success: Loaded {len(self.tickers)} tickers.\")\n",
    "            print(f\"[DataHandler V2] Available tickers: {self.tickers}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DataHandler V2] Error loading data: {e}\")\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        [Data Leakage Î∞©ÏßÄ] ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Î°úÎßå ScalerÎ•º ÌïôÏäµ(fit)\n",
    "        \"\"\"\n",
    "        print(f\"[DataHandler V2] Fitting scalers using data up to {self.train_end_date.date()}...\")\n",
    "        for ticker in self.tickers:\n",
    "            train_data = self.data_by_ticker[ticker].loc[:self.train_end_date]\n",
    "            if train_data.empty:\n",
    "                print(f\"  > Warning: No training data for {ticker}.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_data) # 'fit'ÏùÄ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Î°úÎßå!\n",
    "            self.scalers_by_ticker[ticker] = scaler\n",
    "        print(\"[DataHandler V2] Scalers fitted.\")\n",
    "\n",
    "    def get_scaled_data_by_ticker(self, ticker):\n",
    "        \"\"\"\n",
    "        'transform'ÏùÄ Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê Ï†ÅÏö©ÌïòÏó¨ ÌëúÏ§ÄÌôîÎêú DF Î∞òÌôò\n",
    "        \"\"\"\n",
    "        if ticker not in self.scalers_by_ticker:\n",
    "            print(f\"[DataHandler V2] Error: No scaler for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        original_data = self.data_by_ticker[ticker]\n",
    "        scaler = self.scalers_by_ticker[ticker]\n",
    "        \n",
    "        scaled_data_np = scaler.transform(original_data)\n",
    "        \n",
    "        scaled_df = pd.DataFrame(\n",
    "            scaled_data_np, \n",
    "            index=original_data.index, \n",
    "            columns=original_data.columns\n",
    "        )\n",
    "        return scaled_df\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        return self.tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace50976-1c78-4ee6-bb26-e33c2360544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Í∏∞Î≥∏ import + ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb7283e-4b9f-4819-a170-d12d45516de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT: /workspace/ship-ai\n",
      "[INFO] DATA_DIR    : /workspace/ship-ai/data/processed\n",
      "[INFO] MASTER_TBL  : /workspace/ship-ai/data/processed/final_master_table_v2.csv\n",
      "[INFO] GPT2_PATH   : /workspace/ship-ai/pretrained_models/gpt2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÌïÑÏöîÏãú ÏàòÏ†ï)\n",
    "# ============================================================\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR     = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "GPT2_PATH       = os.path.join(PROJECT_ROOT, \"pretrained_models\", \"gpt2\")\n",
    "TIME_LLM_ROOT   = os.path.join(PROJECT_ROOT, \"external\", \"time-llm\")\n",
    "\n",
    "if TIME_LLM_ROOT not in sys.path:\n",
    "    sys.path.append(TIME_LLM_ROOT)\n",
    "\n",
    "print(\"[INFO] PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"[INFO] DATA_DIR    :\", DATA_DIR)\n",
    "print(\"[INFO] MASTER_TBL  :\", MASTER_TABLE_PATH)\n",
    "print(\"[INFO] GPT2_PATH   :\", GPT2_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d84d1d-3956-4a72-bbe5-b83fd3651874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n",
      "Ïª¨Îüº ÏàúÏÑú ÌôïÏù∏:\n",
      "0: close_log\n",
      "1: ret_1d\n",
      "2: trading_volume_log\n",
      "3: roe\n",
      "4: real_debt_ratio\n",
      "5: new_order_event_impulse\n",
      "6: new_order_count_stair\n",
      "7: bdi_proxy\n",
      "8: wti\n",
      "9: newbuild_proxy_2015_100\n",
      "10: imo_event_impulse\n",
      "11: imo_event_decay\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MASTER_TABLE_PATH = FINAL_MASTER_FILE\n",
    "\n",
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "# ÏïÑÎ¨¥ Ìã∞Ïª§ ÌïòÎÇò Í≥®ÎùºÏÑú (Ïòà: '010140')\n",
    "ticker = '010140'\n",
    "\n",
    "scaled_df = data_handler.get_scaled_data_by_ticker(ticker)\n",
    "\n",
    "print(\"Ïª¨Îüº ÏàúÏÑú ÌôïÏù∏:\")\n",
    "for i, col in enumerate(scaled_df.columns):\n",
    "    print(f\"{i}: {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102e295a-8f5a-4c8c-a7d0-1296d476ebf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOGGING] Training logs will be saved to: /workspace/ship-ai/logs/train_log_20251128_060857.txt\n",
      "[INFO] TimeLLM Î™®Îç∏ ÏûÑÌè¨Ìä∏ ÏÑ±Í≥µ\n",
      "\n",
      "[PHASE 2] DataHandler Ï¥àÍ∏∞Ìôî Î∞è ÏúàÎèÑÏö∞ ÏÉùÏÑ±\n",
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n",
      "[INFO] Ï†ÑÏ≤¥ Ìã∞Ïª§ Ïàò: 6\n",
      "[INFO] Ïòà: ['010140' '010620' '329180' '042660' '443060']\n",
      "  - 010140 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "  - 010620 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 329180\n",
      "  - 042660 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 443060\n",
      "  - 009540 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(794, 120, 12), Y=(794, 10, 12)\n",
      "\n",
      "[INFO] ÌÜµÌï© ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞: (4571, 120, 12) (4571, 10, 12)\n",
      "[SPLIT] train=(3199, 120, 12), val=(457, 120, 12), test=(915, 120, 12)\n",
      "[LOADER] train=400 batches, val=58, test=115\n",
      "\n",
      "[PHASE 2] DataHandler Ï¥àÍ∏∞Ìôî Î∞è ÏúàÎèÑÏö∞ ÏÉùÏÑ±\n",
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n",
      "[INFO] Ï†ÑÏ≤¥ Ìã∞Ïª§ Ïàò: 6\n",
      "[INFO] Ïòà: ['010140' '010620' '329180' '042660' '443060']\n",
      "  - 010140 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "  - 010620 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 329180\n",
      "  - 042660 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 443060\n",
      "  - 009540 ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X=(794, 120, 12), Y=(794, 10, 12)\n",
      "\n",
      "[INFO] ÌÜµÌï© ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞: (4571, 120, 12) (4571, 10, 12)\n",
      "[SPLIT] train=(3199, 120, 12), val=(457, 120, 12), test=(915, 120, 12)\n",
      "[LOADER] train=400 batches, val=58, test=115\n",
      "\n",
      "[MODEL] Trainable params: 67.74M\n",
      "[MODEL] Using patch_len=8, stride=4, llm_layers=8\n",
      "üß™ Sanity Check Start!\n",
      "   > Input Shape: torch.Size([8, 120, 12])\n",
      "   > Target Shape: torch.Size([8, 10, 12])\n",
      "üî• Overfitting on one batch...\n",
      "Epoch 20/100 - Loss: 0.72417569\n",
      "Epoch 40/100 - Loss: 0.31809551\n",
      "Epoch 60/100 - Loss: 0.09086002\n",
      "Epoch 80/100 - Loss: 0.06810091\n",
      "Epoch 100/100 - Loss: 0.07378487\n",
      "\n",
      "üëÄ Visualizing Results...\n",
      "\n",
      "[MODEL] Trainable params: 67.74M\n",
      "[MODEL] Using patch_len=8, stride=4, llm_layers=8\n",
      "fixed_x shape: torch.Size([1, 120, 12]), fixed_y shape: torch.Size([1, 10, 12])\n",
      "üß™ Sanity Check Start!\n",
      "   > Input Shape: torch.Size([1, 120, 12])\n",
      "   > Target Shape: torch.Size([1, 10, 12])\n",
      "üî• Overfitting on one batch...\n",
      "Epoch 20/100 - Loss: 0.89834136\n",
      "Epoch 40/100 - Loss: 0.19942752\n",
      "Epoch 60/100 - Loss: 0.70172471\n",
      "Epoch 80/100 - Loss: 0.22855945\n",
      "Epoch 100/100 - Loss: 0.06088715\n",
      "\n",
      "üëÄ Visualizing Results...\n",
      "fixed_x shape: torch.Size([1, 120, 12]), fixed_y shape: torch.Size([1, 10, 12])\n",
      "üß™ Sanity Check Start!\n",
      "   > Input Shape: torch.Size([1, 120, 12])\n",
      "   > Target Shape: torch.Size([1, 10, 12])\n",
      "üî• Overfitting on one batch...\n",
      "Epoch 20/500 - Loss: 0.04297123\n",
      "Epoch 40/500 - Loss: 0.03585382\n",
      "Epoch 60/500 - Loss: 0.12895826\n",
      "Epoch 80/500 - Loss: 0.08324964\n",
      "Epoch 100/500 - Loss: 0.10169058\n",
      "Epoch 120/500 - Loss: 0.00939631\n",
      "Epoch 140/500 - Loss: 0.01141125\n",
      "Epoch 160/500 - Loss: 0.00309785\n",
      "Epoch 180/500 - Loss: 0.00449467\n",
      "Epoch 200/500 - Loss: 0.00378954\n",
      "Epoch 220/500 - Loss: 0.05089529\n",
      "Epoch 240/500 - Loss: 0.00214419\n",
      "Epoch 260/500 - Loss: 0.00720072\n",
      "Epoch 280/500 - Loss: 0.00695247\n",
      "Epoch 300/500 - Loss: 0.00227160\n",
      "Epoch 320/500 - Loss: 0.00099137\n",
      "Epoch 340/500 - Loss: 0.00218460\n",
      "Epoch 360/500 - Loss: 0.00269272\n",
      "Epoch 380/500 - Loss: 0.00161369\n",
      "Epoch 400/500 - Loss: 0.00118318\n",
      "Epoch 420/500 - Loss: 0.00091923\n",
      "Epoch 440/500 - Loss: 0.00223953\n",
      "Epoch 460/500 - Loss: 0.00173344\n",
      "Epoch 480/500 - Loss: 0.00393568\n",
      "Epoch 500/500 - Loss: 0.00059655\n",
      "\n",
      "üëÄ Visualizing Results...\n",
      "\n",
      "[MODEL] Trainable params: 67.74M\n",
      "[MODEL] Using patch_len=8, stride=4, llm_layers=8\n",
      "[TRAIN] Start training (Horizon=1 Regression)...\n",
      "  > LR=0.0001, EPOCHS=30, ACCUM_STEPS=8\n",
      "[TRAIN] Start training (Horizon=1 Regression)...\n",
      "  > LR=0.0001, EPOCHS=20, ACCUM_STEPS=8\n",
      "[Epoch 1] Avg Loss (h=1 only): 0.24250\n",
      "[Epoch 2] Avg Loss (h=1 only): 0.07552\n",
      "[Epoch 3] Avg Loss (h=1 only): 0.05770\n",
      "[Epoch 4] Avg Loss (h=1 only): 0.04891\n",
      "[Epoch 5] Avg Loss (h=1 only): 0.04426\n",
      "[Epoch 6] Avg Loss (h=1 only): 0.04301\n",
      "[Epoch 7] Avg Loss (h=1 only): 0.03369\n",
      "[Epoch 8] Avg Loss (h=1 only): 0.03557\n",
      "[Epoch 9] Avg Loss (h=1 only): 0.03219\n",
      "[Epoch 10] Avg Loss (h=1 only): 0.02901\n",
      "[Epoch 11] Avg Loss (h=1 only): 0.02666\n",
      "[Epoch 12] Avg Loss (h=1 only): 0.02968\n",
      "[Epoch 13] Avg Loss (h=1 only): 0.02719\n",
      "[Epoch 14] Avg Loss (h=1 only): 0.02535\n",
      "[Epoch 15] Avg Loss (h=1 only): 0.02333\n",
      "[Epoch 16] Avg Loss (h=1 only): 0.02254\n",
      "[Epoch 17] Avg Loss (h=1 only): 0.02177\n",
      "[Epoch 18] Avg Loss (h=1 only): 0.02176\n",
      "[Epoch 19] Avg Loss (h=1 only): 0.02169\n",
      "[Epoch 20] Avg Loss (h=1 only): 0.02113\n",
      "\n",
      "[SAVE] Model saved to: /workspace/ship-ai/models/ship_time_llm_tmp6.pth\n",
      "\n",
      "[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä ÏãúÏûë\n",
      "[train] MSE   model=0.6334, naive=0.1201\n",
      "[train] DIR%  model=49.77%, naive=47.87%\n",
      "------------------------------------------------------------\n",
      "[val] MSE   model=1.1454, naive=0.1515\n",
      "[val] DIR%  model=49.95%, naive=47.73%\n",
      "------------------------------------------------------------\n",
      "== [val] HorizonÎ≥Ñ MSE (h=0ÏùÄ +1ÏùºÏ∞®) ===\n",
      "h+1: MSE_model=0.0366, MSE_naive=0.0307\n",
      "h+2: MSE_model=0.9095, MSE_naive=0.0604\n",
      "h+3: MSE_model=1.3725, MSE_naive=0.0903\n",
      "h+4: MSE_model=0.9586, MSE_naive=0.1211\n",
      "h+5: MSE_model=1.0116, MSE_naive=0.1471\n",
      "h+6: MSE_model=1.5007, MSE_naive=0.1702\n",
      "h+7: MSE_model=1.4636, MSE_naive=0.1908\n",
      "h+8: MSE_model=1.5669, MSE_naive=0.2122\n",
      "h+9: MSE_model=1.2906, MSE_naive=0.2336\n",
      "h+10: MSE_model=1.3434, MSE_naive=0.2585\n",
      "\n",
      "=== [val] HorizonÎ≥Ñ Î∞©Ìñ• Ï†ïÌôïÎèÑ (1~9Ïùº Íµ¨Í∞Ñ) ===\n",
      "Íµ¨Í∞Ñ 1->2: DIR_model=50.21%, DIR_naive=51.40%\n",
      "Íµ¨Í∞Ñ 2->3: DIR_model=53.43%, DIR_naive=43.78%\n",
      "Íµ¨Í∞Ñ 3->4: DIR_model=48.11%, DIR_naive=45.95%\n",
      "Íµ¨Í∞Ñ 4->5: DIR_model=49.51%, DIR_naive=47.71%\n",
      "Íµ¨Í∞Ñ 5->6: DIR_model=53.56%, DIR_naive=45.93%\n",
      "Íµ¨Í∞Ñ 6->7: DIR_model=46.36%, DIR_naive=49.11%\n",
      "Íµ¨Í∞Ñ 7->8: DIR_model=51.75%, DIR_naive=46.52%\n",
      "Íµ¨Í∞Ñ 8->9: DIR_model=49.32%, DIR_naive=49.52%\n",
      "Íµ¨Í∞Ñ 9->10: DIR_model=47.72%, DIR_naive=49.45%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. (ÏÑ†ÌÉù) ÏΩòÏÜî Î°úÍ∑∏Î•º ÌååÏùºÎ°úÎèÑ Ï†ÄÏû•ÌïòÎäî Logger ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_PATH = os.path.join(LOG_DIR, f\"train_log_{timestamp}.txt\")\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(file_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = Logger(LOG_PATH)\n",
    "print(f\"[LOGGING] Training logs will be saved to: {LOG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e748c2-ccae-4d3a-8635-87f214a35a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. TimeLLM Î™®Îç∏ import\n",
    "# ============================================================\n",
    "try:\n",
    "    import importlib\n",
    "    import models.TimeLLM\n",
    "    importlib.reload(models.TimeLLM)\n",
    "    from models.TimeLLM import Model as TimeLLM\n",
    "    print(\"[INFO] TimeLLM Î™®Îç∏ ÏûÑÌè¨Ìä∏ ÏÑ±Í≥µ\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] TimeLLM import Ïã§Ìå®:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda11788-e420-4c5c-95de-653c00783b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Ïä¨ÎùºÏù¥Îî© ÏúàÎèÑÏö∞ Ìï®Ïàò + Dataset Ï†ïÏùò\n",
    "# ============================================================\n",
    "def create_sliding_windows(data, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    DataFrame(2D: [time, features]) -> (X, y) 3D numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôò\n",
    "    X: (N, input_seq_len, C)\n",
    "    y: (N, output_seq_len, C)\n",
    "    \"\"\"\n",
    "    data_np = data.values\n",
    "    n_samples = len(data_np)\n",
    "    X, y = [], []\n",
    "\n",
    "    total_len = input_seq_len + output_seq_len\n",
    "    for i in range(n_samples - total_len + 1):\n",
    "        x_win = data_np[i : i + input_seq_len]\n",
    "        y_win = data_np[i + input_seq_len : i + total_len]\n",
    "        X.append(x_win)\n",
    "        y.append(y_win)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.Y = torch.FloatTensor(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f3d098-819c-43a1-83b8-f49d94d19abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Phase 2: DataHandler ‚Üí Ï†Ñ Ï¢ÖÎ™© ÏúàÎèÑÏö∞ ÏÉùÏÑ± ‚Üí Train/Val/Test Î∂ÑÌï†\n",
    "# ============================================================\n",
    "print(\"\\n[PHASE 2] DataHandler Ï¥àÍ∏∞Ìôî Î∞è ÏúàÎèÑÏö∞ ÏÉùÏÑ±\")\n",
    "\n",
    "# 6-1) DataHandler Ï¥àÍ∏∞Ìôî\n",
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "INPUT_SEQ_LEN  = 120\n",
    "OUTPUT_SEQ_LEN = 10\n",
    "\n",
    "X_all_list = []\n",
    "Y_all_list = []\n",
    "\n",
    "tickers = data_handler.get_all_tickers()\n",
    "print(\"[INFO] Ï†ÑÏ≤¥ Ìã∞Ïª§ Ïàò:\", len(tickers))\n",
    "print(\"[INFO] Ïòà:\", tickers[:5])\n",
    "\n",
    "for t in tickers:\n",
    "    df_scaled = data_handler.get_scaled_data_by_ticker(t)\n",
    "    if df_scaled is None or len(df_scaled) < INPUT_SEQ_LEN + OUTPUT_SEQ_LEN:\n",
    "        continue\n",
    "\n",
    "    X_t, Y_t = create_sliding_windows(df_scaled, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN)\n",
    "    X_all_list.append(X_t)\n",
    "    Y_all_list.append(Y_t)\n",
    "    print(f\"  - {t} ÏúàÎèÑÏö∞ ÏÉùÏÑ±: X={X_t.shape}, Y={Y_t.shape}\")\n",
    "\n",
    "X_all = np.concatenate(X_all_list, axis=0)\n",
    "Y_all = np.concatenate(Y_all_list, axis=0)\n",
    "print(\"\\n[INFO] ÌÜµÌï© ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞:\", X_all.shape, Y_all.shape)  # (N, 120, C), (N, 10, C)\n",
    "\n",
    "# 6-2) ÏãúÍ∞Ñ ÏàúÏÑú Í∑∏ÎåÄÎ°ú 7:1:2 Î∂ÑÌï†\n",
    "total_samples = len(X_all)\n",
    "train_size = int(total_samples * 0.7)\n",
    "val_size   = int(total_samples * 0.1)\n",
    "test_size  = total_samples - train_size - val_size\n",
    "\n",
    "X_train = X_all[:train_size]\n",
    "Y_train = Y_all[:train_size]\n",
    "\n",
    "X_val   = X_all[train_size:train_size+val_size]\n",
    "Y_val   = Y_all[train_size:train_size+val_size]\n",
    "\n",
    "X_test  = X_all[train_size+val_size:]\n",
    "Y_test  = Y_all[train_size+val_size:]\n",
    "\n",
    "print(f\"[SPLIT] train={X_train.shape}, val={X_val.shape}, test={X_test.shape}\")\n",
    "\n",
    "# 6-3) Dataset / DataLoader ÏÉùÏÑ±\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = ShipDataset(X_train, Y_train)\n",
    "val_dataset   = ShipDataset(X_val,   Y_val)\n",
    "test_dataset  = ShipDataset(X_test,  Y_test)\n",
    "\n",
    "train_loader_global = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_global   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_global  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"[LOADER] train={len(train_loader_global)} batches, val={len(val_loader_global)}, test={len(test_loader_global)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92405db4-6148-4b9f-bbfd-ae645246d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        # Í∏∞Î≥∏ ÏÑ∏ÌåÖ\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "\n",
    "        # Îç∞Ïù¥ÌÑ∞ Ï∞®Ïõê\n",
    "        self.seq_len   = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len  = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "\n",
    "        # [ÌïµÏã¨ Î≥ÄÍ≤Ω 1] LLM Î™®Îç∏ ÌíÄÌååÏõå Í∞ÄÎèô (RTX 5090Ïù¥ÎãàÍπå!)\n",
    "        self.llm_model       = 'GPT2'\n",
    "        self.llm_model_path = GPT2_PATH\n",
    "        self.llm_dim    = 768\n",
    "        self.llm_layers = 8     # (Í∏∞Ï°¥ 6 -> 12 Î≥µÍµ¨: ÏßÄÎä• 2Î∞∞)\n",
    "\n",
    "        # [ÌïµÏã¨ Î≥ÄÍ≤Ω 2] ÌòÑÎØ∏Í≤Ω Î™®Îìú (High Resolution)\n",
    "        # 32ÏùºÏî© ÎåÄÏ∂© Î≥¥Îäî Í≤å ÏïÑÎãàÎùº, 8ÏùºÏî© Ï™ºÍ∞úÏÑú ÎîîÌÖåÏùºÏùÑ Ïû°ÏäµÎãàÎã§.\n",
    "        self.patch_len = 8       # (Í∏∞Ï°¥ 32 -> 8: Ìï¥ÏÉÅÎèÑ 4Î∞∞)\n",
    "        self.stride    = 4       # (Í∏∞Ï°¥ 16 -> 4: Îçî Ï¥òÏ¥òÌïòÍ≤å)\n",
    "\n",
    "        # [ÌïµÏã¨ Î≥ÄÍ≤Ω 3] Î™®Îç∏ Îç©Ïπò ÌÇ§Ïö∞Í∏∞\n",
    "        self.d_model = 512      # (Í∏∞Ï°¥ 256 -> 768)\n",
    "        self.d_ff    = 512       # (Í∏∞Ï°¥ 256 -> 768, Ï∞®Ïõê ÏóêÎü¨ Î∞©ÏßÄÏö© ÎèôÍ∏∞Ìôî)\n",
    "        self.n_heads = 12        # (Í∏∞Ï°¥ 12 Ïú†ÏßÄ)\n",
    "        self.dropout = 0.05     # (0.02 -> 0.05: Î™®Îç∏Ïù¥ Ïª§Ï†∏ÏÑú Í∑úÏ†ú ÏÇ¥Ïßù Ï∂îÍ∞Ä)\n",
    "\n",
    "        # Prompt / ÎèÑÎ©îÏù∏ ÏÑ§Î™Ö (Rich Prompt Ïú†ÏßÄ)\n",
    "        self.prompt_domain = 1\n",
    "        self.content = (\n",
    "            \"Task: Forecast daily closing prices for Korean shipbuilding companies. \"\n",
    "            \"Input Data: 12 channels including OHLC prices, trading volume, \"\n",
    "            \"and macro-indicators such as Brent oil price, USD/KRW exchange rate, \"\n",
    "            \"interest rate, and BDI (Baltic Dry Index). \"\n",
    "            \"Context: Shipbuilding stocks are sensitive to oil prices and BDI. \"\n",
    "            \"Analyze the 120-day trend, focusing on volatility and correlations, \"\n",
    "            \"and predict the next 10 days.\"\n",
    "        )\n",
    "\n",
    "        # Í∏∞ÌÉÄ ÏÑ§Ï†ï\n",
    "        self.embed   = 'timeF'\n",
    "        self.freq    = 'd'\n",
    "        self.factor  = 1\n",
    "        self.moving_avg = 25\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.top_k    = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a12648-a2fd-40bd-b2ea-c3e145aafa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 8. Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "# ============================================================\n",
    "configs = Configs()\n",
    "model = TimeLLM(configs)\n",
    "model.to(device).float()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n[MODEL] Trainable params: {n_params/1e6:.2f}M\")\n",
    "print(f\"[MODEL] Using patch_len={configs.patch_len}, stride={configs.stride}, llm_layers={configs.llm_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dc6e68c-8fed-47df-aa84-657925ab469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. ÌïôÏäµ ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS        = 30\n",
    "ACCUM_STEPS   = 8\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = DirectionalMSELoss(direction_weight=5.0)\n",
    "\n",
    "print(\"\\n[TRAIN] Start training...\")\n",
    "print(f\"  > LR={LEARNING_RATE}, EPOCHS={EPOCHS}, ACCUM_STEPS={ACCUM_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d0296e1-53b0-41dc-8d25-bd9999e1bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.01it/s, loss=0.03388, lr=0.000100]\n",
      "Epoch 2/20: 100%|‚ñà‚ñà| 400/400 [01:38<00:00,  4.05it/s, loss=0.03825, lr=0.000099]\n",
      "Epoch 3/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.04it/s, loss=0.01083, lr=0.000098]\n",
      "Epoch 4/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.01015, lr=0.000095]\n",
      "Epoch 5/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.01132, lr=0.000090]\n",
      "Epoch 6/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.17693, lr=0.000085]\n",
      "Epoch 7/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.02291, lr=0.000079]\n",
      "Epoch 8/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.01686, lr=0.000073]\n",
      "Epoch 9/20: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.01583, lr=0.000065]\n",
      "Epoch 10/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.02338, lr=0.000058]\n",
      "Epoch 11/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.03365, lr=0.000050]\n",
      "Epoch 12/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.00639, lr=0.000042]\n",
      "Epoch 13/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.05849, lr=0.000035]\n",
      "Epoch 14/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.03418, lr=0.000027]\n",
      "Epoch 15/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.10831, lr=0.000021]\n",
      "Epoch 16/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.00754, lr=0.000015]\n",
      "Epoch 17/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.00493, lr=0.000010]\n",
      "Epoch 18/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.01822, lr=0.000005]\n",
      "Epoch 19/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.01it/s, loss=0.02056, lr=0.000002]\n",
      "Epoch 20/20: 100%|‚ñà| 400/400 [01:39<00:00,  4.02it/s, loss=0.00310, lr=0.000001]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0. ÏÜêÏã§Ìï®Ïàò / ÏòµÌã∞ÎßàÏù¥Ï†Ä / Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ∏ÌåÖ (ÏòàÏãú)\n",
    "# --------------------------------------------------\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Horizon = 1 ÌöåÍ∑Ä ÌïôÏäµ Î£®ÌîÑ\n",
    "#    - target: batch_y[:, 0, 0]  (ÎÇ¥Ïùº close_log)\n",
    "#    - pred  : outputs[:, 0, 0]  (Î™®Îç∏Ïù¥ ÏòàÏ∏°Ìïú ÎÇ¥Ïùº close_log)\n",
    "# --------------------------------------------------\n",
    "print(\"[TRAIN] Start training (Horizon=1 Regression)...\")\n",
    "print(f\"  > LR={LEARNING_RATE}, EPOCHS={EPOCHS}, ACCUM_STEPS={ACCUM_STEPS}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(train_loader_global, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in enumerate(progress_bar):\n",
    "        batch_x = batch_x.to(device).float()   # (B, 120, C)\n",
    "        batch_y = batch_y.to(device).float()   # (B, 10,  C)\n",
    "\n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]                # ÏõêÎûò 10\n",
    "\n",
    "        # ÎçîÎØ∏ time feature (Ïö∞Î¶¨Îäî Ïïà Ïç®ÏÑú 0ÏúºÎ°ú Îë†)\n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4,  device=device)\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "        dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "        # ---- Î™®Îç∏ forward ----\n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]               # TimeLLM ÏùºÎ∂Ä Î≤ÑÏ†ÑÏùÄ (out, attn) ÌäúÌîåÏùÑ Î∞òÌôòÌï®\n",
    "\n",
    "        # outputs: (B, pred_len, C)  Ïó¨Í∏∞ÏÑú pred_len=10\n",
    "        # Ïö∞Î¶¨Îäî +1Ïùº(h=0), 0Î≤à Ï±ÑÎÑê(close_log)Îßå ÏÇ¨Ïö©\n",
    "        pred_next = outputs[:, 0, 0]           # (B,)\n",
    "        true_next = batch_y[:, 0, 0]           # (B,)\n",
    "\n",
    "        # ---- Loss Í≥ÑÏÇ∞ (Horizon=1 ÌöåÍ∑Ä) ----\n",
    "        loss = criterion(pred_next, true_next)\n",
    "\n",
    "        # ---- Gradient Accumulation ----\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        current_loss = loss.item() * ACCUM_STEPS\n",
    "        total_loss  += current_loss\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix(\n",
    "            {'loss': f\"{current_loss:.5f}\", 'lr': f\"{current_lr:.6f}\"}\n",
    "        )\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader_global)\n",
    "    print(f\"[Epoch {epoch}] Avg Loss (h=1 only): {avg_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a78b32e-8724-4088-be22-831ffc5441ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|‚ñà‚ñà| 400/400 [01:40<00:00,  3.99it/s, loss=0.18332, lr=0.000100]\n",
      "Epoch 2/30: 100%|‚ñà‚ñà| 400/400 [01:38<00:00,  4.05it/s, loss=0.07885, lr=0.000100]\n",
      "Epoch 3/30: 100%|‚ñà‚ñà| 400/400 [01:38<00:00,  4.04it/s, loss=0.00916, lr=0.000099]\n",
      "Epoch 4/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.04it/s, loss=0.18374, lr=0.000098]\n",
      "Epoch 5/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.75705, lr=0.000096]\n",
      "Epoch 6/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.05006, lr=0.000093]\n",
      "Epoch 7/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.06173, lr=0.000090]\n",
      "Epoch 8/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.02755, lr=0.000087]\n",
      "Epoch 9/30: 100%|‚ñà‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.03544, lr=0.000083]\n",
      "Epoch 10/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.27663, lr=0.000079]\n",
      "Epoch 11/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.04803, lr=0.000075]\n",
      "Epoch 12/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.08766, lr=0.000070]\n",
      "Epoch 13/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.05497, lr=0.000065]\n",
      "Epoch 14/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.05417, lr=0.000060]\n",
      "Epoch 15/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.10806, lr=0.000055]\n",
      "Epoch 16/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.18446, lr=0.000050]\n",
      "Epoch 17/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.03489, lr=0.000045]\n",
      "Epoch 18/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.12094, lr=0.000040]\n",
      "Epoch 19/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.04142, lr=0.000035]\n",
      "Epoch 20/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.06808, lr=0.000030]\n",
      "Epoch 21/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.03339, lr=0.000025]\n",
      "Epoch 22/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.06260, lr=0.000021]\n",
      "Epoch 23/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.00623, lr=0.000017]\n",
      "Epoch 24/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.04494, lr=0.000013]\n",
      "Epoch 25/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.12035, lr=0.000010]\n",
      "Epoch 26/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.07049, lr=0.000007]\n",
      "Epoch 27/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.00321, lr=0.000004]\n",
      "Epoch 28/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.06497, lr=0.000002]\n",
      "Epoch 29/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.05833, lr=0.000001]\n",
      "Epoch 30/30: 100%|‚ñà| 400/400 [01:39<00:00,  4.03it/s, loss=0.04020, lr=0.000000]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. ÌïôÏäµ Î£®ÌîÑ (DirectionalMSELoss Î≤ÑÏ†Ñ)\n",
    "# ============================================================\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(train_loader_global, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in enumerate(progress_bar):\n",
    "        batch_x = batch_x.to(device).float()  # (B, 120, C)\n",
    "        batch_y = batch_y.to(device).float()  # (B, 10,  C)\n",
    "\n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]\n",
    "\n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4,  device=device)\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "        dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "        # --- Î™®Îç∏ forward ---\n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "\n",
    "        # outputs: (B, pred_len, C)\n",
    "        # Ïö∞Î¶¨Îäî 0Î≤à Ï±ÑÎÑê(close_log), +1Ïùº(h=0)Îßå ÏÇ¨Ïö©\n",
    "        preds_full = outputs[:, -configs.pred_len:, :]   # (B, Pred, C)\n",
    "        pred_next  = preds_full[:, 0, 0]                 # (B,) ÎÇ¥Ïùº ÏòàÏ∏° close_log\n",
    "        true_next  = batch_y[:, 0, 0]                    # (B,) ÎÇ¥Ïùº Ïã§Ï†ú close_log\n",
    "\n",
    "        last_val   = batch_x[:, -1, 0]                   # (B,) Ïò§Îäò close_log\n",
    "\n",
    "        # --- DirectionalMSE loss ---\n",
    "        loss = criterion(pred_next, true_next, last_val)\n",
    "        # ----------------------------\n",
    "\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        current_loss = loss.item() * ACCUM_STEPS\n",
    "        total_loss  += current_loss\n",
    "        current_lr   = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix({'loss': f\"{current_loss:.5f}\", 'lr': f\"{current_lr:.6f}\"})\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader_global)\n",
    "    print(f\"[Epoch {epoch}] Avg Loss: {avg_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4733253-3521-4d7d-8107-f4db33be1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. Î™®Îç∏ Ï†ÄÏû•\n",
    "# ============================================================\n",
    "SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, \"ship_time_llm_tmp6.pth\")\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\n[SAVE] Model saved to: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7442043d-3658-4341-a99a-1038792eeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Direction Classification Evaluation (DirectionalMSE Î≤ÑÏ†ÑÏö©)\n",
    "#  - True: ÎÇ¥Ïùº Ï¢ÖÍ∞ÄÍ∞Ä Ïò§ÎäòÎ≥¥Îã§ ÌÅ¨Î©¥ 1, ÏïÑÎãàÎ©¥ 0\n",
    "#  - Model: pred_next - last_val Ïùò Î∂ÄÌò∏Î°ú ÏÉÅÏäπ/ÌïòÎùΩ ÌåêÎã®\n",
    "#  - Naive: Ïò§Îäò ÏàòÏùµÎ•†(Ïò§Îäò-Ïñ¥Ï†ú)Ïùò Î∂ÄÌò∏Î°ú ÎÇ¥Ïùº Î∞©Ìñ• Í∞ÄÏ†ï\n",
    "# ============================================================\n",
    "def eval_direction(loader, name=\"train\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    # Î™®Îç∏/naive Í∞ÅÍ∞Å TP/FP/TN/FN\n",
    "    TP_m = FP_m = TN_m = FN_m = 0\n",
    "    TP_n = FP_n = TN_n = FN_n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()  # (B, 120, C)\n",
    "            batch_y = batch_y.to(device).float()  # (B, 10,  C)\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            # ---------- 1) True Label (y_bin) ----------\n",
    "            close_today    = batch_x[:, -1, 0]   # (B,)\n",
    "            close_tomorrow = batch_y[:, 0, 0]    # (B,)\n",
    "\n",
    "            diff = close_tomorrow - close_today\n",
    "            y_bin = (diff > 0).float()          # ÏÉÅÏäπ=1, ÌïòÎùΩ/Ìö°Î≥¥=0\n",
    "\n",
    "            # ---------- 2) Î™®Îç∏ ÏòàÏ∏° ----------\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # outputs: (B, pred_len, C)\n",
    "            preds_full = outputs[:, -configs.pred_len:, :]   # (B, Pred, C)\n",
    "            pred_next  = preds_full[:, 0, 0]                 # (B,) ÎÇ¥Ïùº ÏòàÏ∏° close_log\n",
    "\n",
    "            # Î™®Îç∏Ïùò Î∞©Ìñ•: pred_next - close_today\n",
    "            dir_m = pred_next - close_today                  # (B,)\n",
    "            y_hat_m = (dir_m > 0).float()                    # ÏÉÅÏäπ=1, ÌïòÎùΩ/Ìö°Î≥¥=0\n",
    "\n",
    "            # ---------- 3) Naive ÏòàÏ∏° ----------\n",
    "            # Ïñ¥Ï†ú‚ÜíÏò§Îäò ÏàòÏùµÎ•†Ïùò Î∂ÄÌò∏Î•º ÎÇ¥ÏùºÏóêÎèÑ Í∑∏ÎåÄÎ°ú Í∞ÑÎã§Í≥† Í∞ÄÏ†ï\n",
    "            last_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0] # (B,)\n",
    "            y_hat_n  = (last_ret > 0).float()                # (B,)\n",
    "\n",
    "            # ---------- 4) TP/FP/TN/FN Ïπ¥Ïö¥Ìä∏ ----------\n",
    "            total += B\n",
    "\n",
    "            # Î™®Îç∏\n",
    "            TP_m += ((y_hat_m == 1) & (y_bin == 1)).sum().item()\n",
    "            FP_m += ((y_hat_m == 1) & (y_bin == 0)).sum().item()\n",
    "            TN_m += ((y_hat_m == 0) & (y_bin == 0)).sum().item()\n",
    "            FN_m += ((y_hat_m == 0) & (y_bin == 1)).sum().item()\n",
    "\n",
    "            # naive\n",
    "            TP_n += ((y_hat_n == 1) & (y_bin == 1)).sum().item()\n",
    "            FP_n += ((y_hat_n == 1) & (y_bin == 0)).sum().item()\n",
    "            TN_n += ((y_hat_n == 0) & (y_bin == 0)).sum().item()\n",
    "            FN_n += ((y_hat_n == 0) & (y_bin == 1)).sum().item()\n",
    "\n",
    "    def metrics(TP, FP, TN, FN):\n",
    "        denom = max(TP + FP + TN + FN, 1)\n",
    "        acc   = (TP + TN) / denom\n",
    "        prec  = TP / max(TP + FP, 1)\n",
    "        rec   = TP / max(TP + FN, 1)\n",
    "        return acc, prec, rec\n",
    "\n",
    "    acc_m, prec_m, rec_m = metrics(TP_m, FP_m, TN_m, FN_m)\n",
    "    acc_n, prec_n, rec_n = metrics(TP_n, FP_n, TN_n, FN_n)\n",
    "\n",
    "    print(f\"\\n=== [{name}] Direction Evaluation ===\")\n",
    "    print(f\"Ï¥ù ÏÉòÌîå Ïàò: {total}\")\n",
    "    print(f\"[Model]  Acc={acc_m*100:5.2f}%  Prec={prec_m*100:5.2f}%  Rec={rec_m*100:5.2f}%\")\n",
    "    print(f\"[Naive]  Acc={acc_n*100:5.2f}%  Prec={prec_n*100:5.2f}%  Rec={rec_n*100:5.2f}%\")\n",
    "    print(f\"[Model]  TP={TP_m}, FP={FP_m}, TN={TN_m}, FN={FN_m}\")\n",
    "    print(f\"[Naive]  TP={TP_n}, FP={FP_n}, TN={TN_n}, FN={FN_n}\")\n",
    "    print(\"=====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fc3335b-5589-4922-9930-ac45965d7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. ÌèâÍ∞Ä Ïã§Ìñâ ÏòàÏãú\n",
    "# ============================================================\n",
    "print(\"\\n[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä ÏãúÏûë\")\n",
    "eval_direction(train_loader_global, \"train\")\n",
    "eval_direction(val_loader_global, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b23e9392-3137-43ef-a39b-8bca44aa2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. ÌèâÍ∞Ä Ìï®Ïàò (MSE / DIR%) + Horizon Î∂ÑÏÑù\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "def eval_loader(loader, name=\"train\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    mse_model_list = []\n",
    "    mse_naive_list = []\n",
    "    dir_model_list = []\n",
    "    dir_naive_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]  # (B, Pred, C)\n",
    "\n",
    "            true = batch_y[:, :, 0]   # (B, Pred)\n",
    "            pred = preds[:, :, 0]     # (B, Pred)\n",
    "\n",
    "            # 1) MSE\n",
    "            mse_model = torch.mean((pred - true)**2).item()\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)\n",
    "            mse_naive = torch.mean((naive - true)**2).item()\n",
    "\n",
    "            mse_model_list.append(mse_model)\n",
    "            mse_naive_list.append(mse_naive)\n",
    "\n",
    "            # 2) Î∞©Ìñ• Ï†ïÌôïÎèÑ\n",
    "            true_ret = true[:, 1:] - true[:, :-1]\n",
    "            pred_ret = pred[:, 1:] - pred[:, :-1]\n",
    "\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            mask = true_sign != 0\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            acc_m = (pred_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "            acc_n = (naive_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "\n",
    "            dir_model_list.append(acc_m)\n",
    "            dir_naive_list.append(acc_n)\n",
    "\n",
    "    avg_mse_model = np.mean(mse_model_list)\n",
    "    avg_mse_naive = np.mean(mse_naive_list)\n",
    "    avg_dir_model = np.mean(dir_model_list) * 100\n",
    "    avg_dir_naive = np.mean(dir_naive_list) * 100\n",
    "\n",
    "    print(f\"[{name}] MSE   model={avg_mse_model:.4f}, naive={avg_mse_naive:.4f}\")\n",
    "    print(f\"[{name}] DIR%  model={avg_dir_model:.2f}%, naive={avg_dir_naive:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def eval_horizon(loader, name=\"val\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    Pred = configs.pred_len\n",
    "\n",
    "    mse_model_h = [[] for _ in range(Pred)]\n",
    "    mse_naive_h = [[] for _ in range(Pred)]\n",
    "    dir_model_h = [[] for _ in range(Pred-1)]\n",
    "    dir_naive_h = [[] for _ in range(Pred-1)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]\n",
    "\n",
    "            true = batch_y[:, :, 0]\n",
    "            pred = preds[:, :, 0]\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)\n",
    "\n",
    "            # HorizonÎ≥Ñ MSE\n",
    "            for h in range(Pred):\n",
    "                mse_m = torch.mean((pred[:, h] - true[:, h])**2).item()\n",
    "                mse_n = torch.mean((naive[:, h] - true[:, h])**2).item()\n",
    "                mse_model_h[h].append(mse_m)\n",
    "                mse_naive_h[h].append(mse_n)\n",
    "\n",
    "            # HorizonÎ≥Ñ Î∞©Ìñ• Ï†ïÌôïÎèÑ\n",
    "            true_ret  = true[:, 1:] - true[:, :-1]\n",
    "            pred_ret  = pred[:, 1:] - pred[:, :-1]\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            for h in range(Pred-1):\n",
    "                ts = true_sign[:, h]\n",
    "                ps = pred_sign[:, h]\n",
    "                ns = naive_sign[:, h]\n",
    "\n",
    "                mask = ts != 0\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                acc_m = (ps[mask] == ts[mask]).float().mean().item()\n",
    "                acc_n = (ns[mask] == ts[mask]).float().mean().item()\n",
    "\n",
    "                dir_model_h[h].append(acc_m)\n",
    "                dir_naive_h[h].append(acc_n)\n",
    "\n",
    "    mse_model_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_model_h]\n",
    "    mse_naive_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_naive_h]\n",
    "    dir_model_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_model_h]\n",
    "    dir_naive_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_naive_h]\n",
    "\n",
    "    print(f\"== [{name}] HorizonÎ≥Ñ MSE (h=0ÏùÄ +1ÏùºÏ∞®) ===\")\n",
    "    for h in range(Pred):\n",
    "        print(f\"h+{h+1}: MSE_model={mse_model_h[h]:.4f}, MSE_naive={mse_naive_h[h]:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== [{name}] HorizonÎ≥Ñ Î∞©Ìñ• Ï†ïÌôïÎèÑ (1~9Ïùº Íµ¨Í∞Ñ) ===\")\n",
    "    for h in range(Pred-1):\n",
    "        print(f\"Íµ¨Í∞Ñ {h+1}->{h+2}: DIR_model={dir_model_h[h]:.2f}%, DIR_naive={dir_naive_h[h]:.2f}%\")\n",
    "\n",
    "    return mse_model_h, mse_naive_h, dir_model_h, dir_naive_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97525035-412b-4544-915f-bfd049a7d08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(0.036556711821848976),\n",
       "  np.float64(0.9095412556338927),\n",
       "  np.float64(1.3724864285182337),\n",
       "  np.float64(0.9585807690204218),\n",
       "  np.float64(1.0115850683109),\n",
       "  np.float64(1.5007414740574514),\n",
       "  np.float64(1.463560911698331),\n",
       "  np.float64(1.566891599607108),\n",
       "  np.float64(1.2905951568632033),\n",
       "  np.float64(1.3434308393183967)],\n",
       " [np.float64(0.030715870161155432),\n",
       "  np.float64(0.06035709488486614),\n",
       "  np.float64(0.09029256207627598),\n",
       "  np.float64(0.12113065682833307),\n",
       "  np.float64(0.1470897889862656),\n",
       "  np.float64(0.17020487708264384),\n",
       "  np.float64(0.19079851161624337),\n",
       "  np.float64(0.21216568321888818),\n",
       "  np.float64(0.2335972248672925),\n",
       "  np.float64(0.2584585758279367)],\n",
       " [np.float64(50.205255299806595),\n",
       "  np.float64(53.427751079715534),\n",
       "  np.float64(48.111659057181456),\n",
       "  np.float64(49.50738991128987),\n",
       "  np.float64(53.56116662251538),\n",
       "  np.float64(46.35673311763796),\n",
       "  np.float64(51.754926787368184),\n",
       "  np.float64(49.32266065272792),\n",
       "  np.float64(47.72167565493748)],\n",
       " [np.float64(51.39573144501654),\n",
       "  np.float64(43.78078877925873),\n",
       "  np.float64(45.94622398244923),\n",
       "  np.float64(47.711412865540076),\n",
       "  np.float64(45.925698429346085),\n",
       "  np.float64(49.10714356549855),\n",
       "  np.float64(46.5209365661802),\n",
       "  np.float64(49.517652597920645),\n",
       "  np.float64(49.44581350889699)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. ÌèâÍ∞Ä Ïã§Ìñâ ÏòàÏãú\n",
    "# ============================================================\n",
    "print(\"\\n[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä ÏãúÏûë\")\n",
    "eval_loader(train_loader_global, \"train\")\n",
    "eval_loader(val_loader_global,   \"val\")\n",
    "eval_horizon(val_loader_global,  \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14ebf23-c0d8-4399-957f-bf6f79b7b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_h1(loader, name=\"train\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    mse_model_list = []\n",
    "    mse_naive_list = []\n",
    "    dir_model_list = []\n",
    "    dir_naive_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # +1Ïùº / close_logÎßå ÏÇ¨Ïö©\n",
    "            pred_next = outputs[:, 0, 0]   # (B,)\n",
    "            true_next = batch_y[:, 0, 0]   # (B,)\n",
    "\n",
    "            # MSE\n",
    "            mse_model = torch.mean((pred_next - true_next)**2).item()\n",
    "            naive_next = batch_x[:, -1, 0]  # \"ÎÇ¥ÏùºÎèÑ Ïò§ÎäòÏù¥Îûë Í∞ôÎã§Í≥† Í∞ÄÏ†ï\"\n",
    "            mse_naive = torch.mean((naive_next - true_next)**2).item()\n",
    "\n",
    "            mse_model_list.append(mse_model)\n",
    "            mse_naive_list.append(mse_naive)\n",
    "\n",
    "            # Î∞©Ìñ• Ï†ïÌôïÎèÑ (ÏÉÅÏäπ/ÌïòÎùΩ)\n",
    "            true_ret = true_next - batch_x[:, -1, 0]\n",
    "            pred_ret = pred_next - batch_x[:, -1, 0]\n",
    "\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            last_ret  = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_ret)\n",
    "\n",
    "            mask = true_sign != 0\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            acc_m = (pred_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "            acc_n = (naive_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "\n",
    "            dir_model_list.append(acc_m)\n",
    "            dir_naive_list.append(acc_n)\n",
    "\n",
    "    avg_mse_model = np.mean(mse_model_list)\n",
    "    avg_mse_naive = np.mean(mse_naive_list)\n",
    "    avg_dir_model = np.mean(dir_model_list) * 100\n",
    "    avg_dir_naive = np.mean(dir_naive_list) * 100\n",
    "\n",
    "    print(f\"[{name} h=1] MSE   model={avg_mse_model:.4f}, naive={avg_mse_naive:.4f}\")\n",
    "    print(f\"[{name} h=1] DIR%  model={avg_dir_model:.2f}%, naive={avg_dir_naive:.2f}%\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3412aa3a-3723-4122-9ad3-de8eb1c2a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä (h=1 Ï†ÑÏö©)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä (h=1 Ï†ÑÏö©)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m eval_h1(\u001b[43mtrain_loader_global\u001b[49m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m eval_h1(val_loader_global,   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader_global' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"[Eval] Global ÏÑ±Îä• ÌèâÍ∞Ä (h=1 Ï†ÑÏö©)\")\n",
    "eval_h1(train_loader_global, name=\"train\")\n",
    "eval_h1(val_loader_global,   name=\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122fa26-2858-4fd2-b239-f0274002c4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
