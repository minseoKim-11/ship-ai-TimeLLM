{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc247867-0e18-4946-9088-7f31ecabb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# [1] --- DataHandler 클래스 정의 (V2: Scaler + zfill) ---\n",
    "# (Phase 2-A와 2-C가 합쳐진 최종 버전)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 0. 경로 설정 ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    [V2] 표준화(Standardization)와 zfill(6)이 적용된 DataHandler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, train_end_date='2022-12-31'):\n",
    "        self.file_path = file_path\n",
    "        self.train_end_date = pd.to_datetime(train_end_date)\n",
    "        self.data_by_ticker = {}   # 원본 데이터\n",
    "        self.scalers_by_ticker = {} # Ticker별 Scaler\n",
    "        self.tickers = []\n",
    "        \n",
    "        self._load_and_process_data()\n",
    "        self._fit_scalers()\n",
    "        \n",
    "    def _load_and_process_data(self):\n",
    "        try:\n",
    "            # 1. dtype=str로 읽기\n",
    "            df = pd.read_csv(\n",
    "                self.file_path, \n",
    "                parse_dates=['date'],\n",
    "                dtype={'ticker': str} \n",
    "            )\n",
    "            # 2. zfill(6)로 '0' 채우기\n",
    "            df['ticker'] = df['ticker'].str.zfill(6)\n",
    "            df = df.set_index('date')\n",
    "            \n",
    "            self.tickers = df['ticker'].unique()\n",
    "            \n",
    "            for ticker in self.tickers:\n",
    "                ticker_df = df[df['ticker'] == ticker].copy()\n",
    "                channel_cols = [col for col in ticker_df.columns if col not in ['ticker']]\n",
    "                self.data_by_ticker[ticker] = ticker_df[channel_cols]\n",
    "            \n",
    "            print(f\"[DataHandler V2] Success: Loaded {len(self.tickers)} tickers.\")\n",
    "            print(f\"[DataHandler V2] Available tickers: {self.tickers}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DataHandler V2] Error loading data: {e}\")\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        [Data Leakage 방지] 훈련 데이터로만 Scaler를 학습(fit)\n",
    "        \"\"\"\n",
    "        print(f\"[DataHandler V2] Fitting scalers using data up to {self.train_end_date.date()}...\")\n",
    "        for ticker in self.tickers:\n",
    "            train_data = self.data_by_ticker[ticker].loc[:self.train_end_date]\n",
    "            if train_data.empty:\n",
    "                print(f\"  > Warning: No training data for {ticker}.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_data) # 'fit'은 훈련 데이터로만!\n",
    "            self.scalers_by_ticker[ticker] = scaler\n",
    "        print(\"[DataHandler V2] Scalers fitted.\")\n",
    "\n",
    "    def get_scaled_data_by_ticker(self, ticker):\n",
    "        \"\"\"\n",
    "        'transform'은 전체 데이터에 적용하여 표준화된 DF 반환\n",
    "        \"\"\"\n",
    "        if ticker not in self.scalers_by_ticker:\n",
    "            print(f\"[DataHandler V2] Error: No scaler for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        original_data = self.data_by_ticker[ticker]\n",
    "        scaler = self.scalers_by_ticker[ticker]\n",
    "        \n",
    "        scaled_data_np = scaler.transform(original_data)\n",
    "        \n",
    "        scaled_df = pd.DataFrame(\n",
    "            scaled_data_np, \n",
    "            index=original_data.index, \n",
    "            columns=original_data.columns\n",
    "        )\n",
    "        return scaled_df\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        return self.tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace50976-1c78-4ee6-bb26-e33c2360544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. 기본 import + 디바이스 설정\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb7283e-4b9f-4819-a170-d12d45516de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT: /workspace/ship-ai\n",
      "[INFO] DATA_DIR    : /workspace/ship-ai/data/processed\n",
      "[INFO] MASTER_TBL  : /workspace/ship-ai/data/processed/final_master_table_v2.csv\n",
      "[INFO] GPT2_PATH   : /workspace/ship-ai/pretrained_models/gpt2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. 경로 설정 (프로젝트 구조에 맞게 필요시 수정)\n",
    "# ============================================================\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR     = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "GPT2_PATH       = os.path.join(PROJECT_ROOT, \"pretrained_models\", \"gpt2\")\n",
    "TIME_LLM_ROOT   = os.path.join(PROJECT_ROOT, \"external\", \"time-llm\")\n",
    "\n",
    "if TIME_LLM_ROOT not in sys.path:\n",
    "    sys.path.append(TIME_LLM_ROOT)\n",
    "\n",
    "print(\"[INFO] PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"[INFO] DATA_DIR    :\", DATA_DIR)\n",
    "print(\"[INFO] MASTER_TBL  :\", MASTER_TABLE_PATH)\n",
    "print(\"[INFO] GPT2_PATH   :\", GPT2_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102e295a-8f5a-4c8c-a7d0-1296d476ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOGGING] Training logs will be saved to: /workspace/ship-ai/logs/train_log_20251121_125023.txt\n",
      "[INFO] TimeLLM 모델 임포트 성공\n",
      "\n",
      "[PHASE 2] DataHandler 초기화 및 윈도우 생성\n",
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n",
      "[INFO] 전체 티커 수: 6\n",
      "[INFO] 예: ['010140' '010620' '329180' '042660' '443060']\n",
      "  - 010140 윈도우 생성: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "  - 010620 윈도우 생성: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 329180\n",
      "  - 042660 윈도우 생성: X=(1259, 120, 12), Y=(1259, 10, 12)\n",
      "[DataHandler V2] Error: No scaler for 443060\n",
      "  - 009540 윈도우 생성: X=(794, 120, 12), Y=(794, 10, 12)\n",
      "\n",
      "[INFO] 통합 윈도우 크기: (4571, 120, 12) (4571, 10, 12)\n",
      "[SPLIT] train=(3199, 120, 12), val=(457, 120, 12), test=(915, 120, 12)\n",
      "[LOADER] train=400 batches, val=58, test=115\n",
      "\n",
      "[MODEL] Trainable params: 67.74M\n",
      "[MODEL] Using patch_len=8, stride=4, llm_layers=8\n",
      "\n",
      "[TRAIN] Start training...\n",
      "  > LR=0.0001, EPOCHS=30, ACCUM_STEPS=8\n",
      "[Epoch 1] Avg Loss: 0.80132\n",
      "[Epoch 2] Avg Loss: 0.53963\n",
      "[Epoch 3] Avg Loss: 0.51062\n",
      "[Epoch 4] Avg Loss: 0.50020\n",
      "[Epoch 5] Avg Loss: 0.49602\n",
      "[Epoch 6] Avg Loss: 0.49206\n",
      "[Epoch 7] Avg Loss: 0.48331\n",
      "[Epoch 8] Avg Loss: 0.47947\n",
      "[Epoch 9] Avg Loss: 0.47242\n",
      "[Epoch 10] Avg Loss: 0.47153\n",
      "[Epoch 11] Avg Loss: 0.46949\n",
      "[Epoch 12] Avg Loss: 0.46582\n",
      "[Epoch 13] Avg Loss: 0.46344\n",
      "[Epoch 14] Avg Loss: 0.45897\n",
      "[Epoch 15] Avg Loss: 0.45982\n",
      "[Epoch 16] Avg Loss: 0.45614\n",
      "[Epoch 17] Avg Loss: 0.45378\n",
      "[Epoch 18] Avg Loss: 0.45036\n",
      "[Epoch 19] Avg Loss: 0.44959\n",
      "[Epoch 20] Avg Loss: 0.44713\n",
      "[Epoch 21] Avg Loss: 0.44502\n",
      "[Epoch 22] Avg Loss: 0.44264\n",
      "[Epoch 23] Avg Loss: 0.44219\n",
      "[Epoch 24] Avg Loss: 0.43929\n",
      "[Epoch 25] Avg Loss: 0.43928\n",
      "[Epoch 26] Avg Loss: 0.43782\n",
      "[Epoch 27] Avg Loss: 0.43887\n",
      "[Epoch 28] Avg Loss: 0.43647\n",
      "[Epoch 29] Avg Loss: 0.43733\n",
      "[Epoch 30] Avg Loss: 0.43681\n",
      "\n",
      "[SAVE] Model saved to: /workspace/ship-ai/models/ship_time_llm_tmp4.pth\n",
      "\n",
      "[Eval] Global 성능 평가 시작\n",
      "[train] MSE   model=0.1192, naive=0.1201\n",
      "[train] DIR%  model=51.42%, naive=47.86%\n",
      "------------------------------------------------------------\n",
      "[val] MSE   model=0.2012, naive=0.1515\n",
      "[val] DIR%  model=50.18%, naive=47.73%\n",
      "------------------------------------------------------------\n",
      "== [val] Horizon별 MSE (h=0은 +1일차) ===\n",
      "h+1: MSE_model=0.0489, MSE_naive=0.0307\n",
      "h+2: MSE_model=0.0870, MSE_naive=0.0604\n",
      "h+3: MSE_model=0.1191, MSE_naive=0.0903\n",
      "h+4: MSE_model=0.1528, MSE_naive=0.1211\n",
      "h+5: MSE_model=0.1853, MSE_naive=0.1471\n",
      "h+6: MSE_model=0.2262, MSE_naive=0.1702\n",
      "h+7: MSE_model=0.2472, MSE_naive=0.1908\n",
      "h+8: MSE_model=0.2849, MSE_naive=0.2122\n",
      "h+9: MSE_model=0.3114, MSE_naive=0.2336\n",
      "h+10: MSE_model=0.3495, MSE_naive=0.2585\n",
      "\n",
      "=== [val] Horizon별 방향 정확도 (1~9일 구간) ===\n",
      "구간 1->2: DIR_model=48.48%, DIR_naive=51.40%\n",
      "구간 2->3: DIR_model=48.16%, DIR_naive=43.78%\n",
      "구간 3->4: DIR_model=52.77%, DIR_naive=45.95%\n",
      "구간 4->5: DIR_model=48.87%, DIR_naive=47.71%\n",
      "구간 5->6: DIR_model=50.88%, DIR_naive=45.93%\n",
      "구간 6->7: DIR_model=55.38%, DIR_naive=49.11%\n",
      "구간 7->8: DIR_model=51.92%, DIR_naive=46.52%\n",
      "구간 8->9: DIR_model=46.09%, DIR_naive=49.52%\n",
      "구간 9->10: DIR_model=49.14%, DIR_naive=49.45%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. (선택) 콘솔 로그를 파일로도 저장하는 Logger 설정\n",
    "# ============================================================\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_PATH = os.path.join(LOG_DIR, f\"train_log_{timestamp}.txt\")\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(file_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = Logger(LOG_PATH)\n",
    "print(f\"[LOGGING] Training logs will be saved to: {LOG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e748c2-ccae-4d3a-8635-87f214a35a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. TimeLLM 모델 import\n",
    "# ============================================================\n",
    "try:\n",
    "    import importlib\n",
    "    import models.TimeLLM\n",
    "    importlib.reload(models.TimeLLM)\n",
    "    from models.TimeLLM import Model as TimeLLM\n",
    "    print(\"[INFO] TimeLLM 모델 임포트 성공\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] TimeLLM import 실패:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda11788-e420-4c5c-95de-653c00783b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. 슬라이딩 윈도우 함수 + Dataset 정의\n",
    "# ============================================================\n",
    "def create_sliding_windows(data, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    DataFrame(2D: [time, features]) -> (X, y) 3D numpy 배열로 변환\n",
    "    X: (N, input_seq_len, C)\n",
    "    y: (N, output_seq_len, C)\n",
    "    \"\"\"\n",
    "    data_np = data.values\n",
    "    n_samples = len(data_np)\n",
    "    X, y = [], []\n",
    "\n",
    "    total_len = input_seq_len + output_seq_len\n",
    "    for i in range(n_samples - total_len + 1):\n",
    "        x_win = data_np[i : i + input_seq_len]\n",
    "        y_win = data_np[i + input_seq_len : i + total_len]\n",
    "        X.append(x_win)\n",
    "        y.append(y_win)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.Y = torch.FloatTensor(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f3d098-819c-43a1-83b8-f49d94d19abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Phase 2: DataHandler → 전 종목 윈도우 생성 → Train/Val/Test 분할\n",
    "# ============================================================\n",
    "print(\"\\n[PHASE 2] DataHandler 초기화 및 윈도우 생성\")\n",
    "\n",
    "# 6-1) DataHandler 초기화\n",
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n",
    "\n",
    "INPUT_SEQ_LEN  = 120\n",
    "OUTPUT_SEQ_LEN = 10\n",
    "\n",
    "X_all_list = []\n",
    "Y_all_list = []\n",
    "\n",
    "tickers = data_handler.get_all_tickers()\n",
    "print(\"[INFO] 전체 티커 수:\", len(tickers))\n",
    "print(\"[INFO] 예:\", tickers[:5])\n",
    "\n",
    "for t in tickers:\n",
    "    df_scaled = data_handler.get_scaled_data_by_ticker(t)\n",
    "    if df_scaled is None or len(df_scaled) < INPUT_SEQ_LEN + OUTPUT_SEQ_LEN:\n",
    "        continue\n",
    "\n",
    "    X_t, Y_t = create_sliding_windows(df_scaled, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN)\n",
    "    X_all_list.append(X_t)\n",
    "    Y_all_list.append(Y_t)\n",
    "    print(f\"  - {t} 윈도우 생성: X={X_t.shape}, Y={Y_t.shape}\")\n",
    "\n",
    "X_all = np.concatenate(X_all_list, axis=0)\n",
    "Y_all = np.concatenate(Y_all_list, axis=0)\n",
    "print(\"\\n[INFO] 통합 윈도우 크기:\", X_all.shape, Y_all.shape)  # (N, 120, C), (N, 10, C)\n",
    "\n",
    "# 6-2) 시간 순서 그대로 7:1:2 분할\n",
    "total_samples = len(X_all)\n",
    "train_size = int(total_samples * 0.7)\n",
    "val_size   = int(total_samples * 0.1)\n",
    "test_size  = total_samples - train_size - val_size\n",
    "\n",
    "X_train = X_all[:train_size]\n",
    "Y_train = Y_all[:train_size]\n",
    "\n",
    "X_val   = X_all[train_size:train_size+val_size]\n",
    "Y_val   = Y_all[train_size:train_size+val_size]\n",
    "\n",
    "X_test  = X_all[train_size+val_size:]\n",
    "Y_test  = Y_all[train_size+val_size:]\n",
    "\n",
    "print(f\"[SPLIT] train={X_train.shape}, val={X_val.shape}, test={X_test.shape}\")\n",
    "\n",
    "# 6-3) Dataset / DataLoader 생성\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = ShipDataset(X_train, Y_train)\n",
    "val_dataset   = ShipDataset(X_val,   Y_val)\n",
    "test_dataset  = ShipDataset(X_test,  Y_test)\n",
    "\n",
    "train_loader_global = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_global   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_global  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"[LOADER] train={len(train_loader_global)} batches, val={len(val_loader_global)}, test={len(test_loader_global)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92405db4-6148-4b9f-bbfd-ae645246d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        # 기본 세팅\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "\n",
    "        # 데이터 차원\n",
    "        self.seq_len   = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len  = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "\n",
    "        # [핵심 변경 1] LLM 모델 풀파워 가동 (RTX 5090이니까!)\n",
    "        self.llm_model       = 'GPT2'\n",
    "        self.llm_model_path = GPT2_PATH\n",
    "        self.llm_dim    = 768\n",
    "        self.llm_layers = 8     # (기존 6 -> 12 복구: 지능 2배)\n",
    "\n",
    "        # [핵심 변경 2] 현미경 모드 (High Resolution)\n",
    "        # 32일씩 대충 보는 게 아니라, 8일씩 쪼개서 디테일을 잡습니다.\n",
    "        self.patch_len = 8       # (기존 32 -> 8: 해상도 4배)\n",
    "        self.stride    = 4       # (기존 16 -> 4: 더 촘촘하게)\n",
    "\n",
    "        # [핵심 변경 3] 모델 덩치 키우기\n",
    "        self.d_model = 512      # (기존 256 -> 768)\n",
    "        self.d_ff    = 512       # (기존 256 -> 768, 차원 에러 방지용 동기화)\n",
    "        self.n_heads = 12        # (기존 12 유지)\n",
    "        self.dropout = 0.05      # (0.02 -> 0.05: 모델이 커져서 규제 살짝 추가)\n",
    "\n",
    "        # Prompt / 도메인 설명 (Rich Prompt 유지)\n",
    "        self.prompt_domain = 1\n",
    "        self.content = (\n",
    "            \"Task: Forecast daily closing prices for Korean shipbuilding companies. \"\n",
    "            \"Input Data: 12 channels including OHLC prices, trading volume, \"\n",
    "            \"and macro-indicators such as Brent oil price, USD/KRW exchange rate, \"\n",
    "            \"interest rate, and BDI (Baltic Dry Index). \"\n",
    "            \"Context: Shipbuilding stocks are sensitive to oil prices and BDI. \"\n",
    "            \"Analyze the 120-day trend, focusing on volatility and correlations, \"\n",
    "            \"and predict the next 10 days.\"\n",
    "        )\n",
    "\n",
    "        # 기타 설정\n",
    "        self.embed   = 'timeF'\n",
    "        self.freq    = 'd'\n",
    "        self.factor  = 1\n",
    "        self.moving_avg = 25\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.top_k    = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a12648-a2fd-40bd-b2ea-c3e145aafa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 8. 모델 초기화\n",
    "# ============================================================\n",
    "configs = Configs()\n",
    "model = TimeLLM(configs)\n",
    "model.to(device).float()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n[MODEL] Trainable params: {n_params/1e6:.2f}M\")\n",
    "print(f\"[MODEL] Using patch_len={configs.patch_len}, stride={configs.stride}, llm_layers={configs.llm_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc6e68c-8fed-47df-aa84-657925ab469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 9. 학습 설정\n",
    "# ============================================================\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS        = 30\n",
    "ACCUM_STEPS   = 8\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\n[TRAIN] Start training...\")\n",
    "print(f\"  > LR={LEARNING_RATE}, EPOCHS={EPOCHS}, ACCUM_STEPS={ACCUM_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a78b32e-8724-4088-be22-831ffc5441ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██| 400/400 [01:37<00:00,  4.09it/s, loss=0.55239, lr=0.000100]\n",
      "Epoch 2/30: 100%|██| 400/400 [01:37<00:00,  4.09it/s, loss=0.35248, lr=0.000100]\n",
      "Epoch 3/30: 100%|██| 400/400 [01:38<00:00,  4.07it/s, loss=0.50438, lr=0.000099]\n",
      "Epoch 4/30: 100%|██| 400/400 [01:38<00:00,  4.07it/s, loss=0.47037, lr=0.000098]\n",
      "Epoch 5/30: 100%|██| 400/400 [01:38<00:00,  4.06it/s, loss=0.60655, lr=0.000096]\n",
      "Epoch 6/30: 100%|██| 400/400 [01:38<00:00,  4.05it/s, loss=0.33243, lr=0.000093]\n",
      "Epoch 7/30: 100%|██| 400/400 [01:38<00:00,  4.07it/s, loss=0.54228, lr=0.000090]\n",
      "Epoch 8/30: 100%|██| 400/400 [01:38<00:00,  4.06it/s, loss=0.42460, lr=0.000087]\n",
      "Epoch 9/30: 100%|██| 400/400 [01:38<00:00,  4.06it/s, loss=0.41267, lr=0.000083]\n",
      "Epoch 10/30: 100%|█| 400/400 [01:37<00:00,  4.09it/s, loss=0.87315, lr=0.000079]\n",
      "Epoch 11/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.60287, lr=0.000075]\n",
      "Epoch 12/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.56873, lr=0.000070]\n",
      "Epoch 13/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.30134, lr=0.000065]\n",
      "Epoch 14/30: 100%|█| 400/400 [01:38<00:00,  4.08it/s, loss=0.29219, lr=0.000060]\n",
      "Epoch 15/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.47645, lr=0.000055]\n",
      "Epoch 16/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.45721, lr=0.000050]\n",
      "Epoch 17/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.30263, lr=0.000045]\n",
      "Epoch 18/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.26014, lr=0.000040]\n",
      "Epoch 19/30: 100%|█| 400/400 [01:38<00:00,  4.08it/s, loss=0.43304, lr=0.000035]\n",
      "Epoch 20/30: 100%|█| 400/400 [01:37<00:00,  4.09it/s, loss=0.37097, lr=0.000030]\n",
      "Epoch 21/30: 100%|█| 400/400 [01:37<00:00,  4.08it/s, loss=0.43461, lr=0.000025]\n",
      "Epoch 22/30: 100%|█| 400/400 [01:38<00:00,  4.08it/s, loss=0.56176, lr=0.000021]\n",
      "Epoch 23/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.22690, lr=0.000017]\n",
      "Epoch 24/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.31386, lr=0.000013]\n",
      "Epoch 25/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.39563, lr=0.000010]\n",
      "Epoch 26/30: 100%|█| 400/400 [01:37<00:00,  4.08it/s, loss=0.61434, lr=0.000007]\n",
      "Epoch 27/30: 100%|█| 400/400 [01:38<00:00,  4.08it/s, loss=0.41245, lr=0.000004]\n",
      "Epoch 28/30: 100%|█| 400/400 [01:38<00:00,  4.08it/s, loss=0.53780, lr=0.000002]\n",
      "Epoch 29/30: 100%|█| 400/400 [01:38<00:00,  4.06it/s, loss=0.24603, lr=0.000001]\n",
      "Epoch 30/30: 100%|█| 400/400 [01:38<00:00,  4.07it/s, loss=0.50124, lr=0.000000]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. 학습 루프\n",
    "# ============================================================\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(train_loader_global, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    for i, (batch_x, batch_y) in enumerate(progress_bar):\n",
    "        batch_x = batch_x.to(device).float()  # (B, 120, C)\n",
    "        batch_y = batch_y.to(device).float()  # (B, 10,  C)\n",
    "\n",
    "        B, Seq, C = batch_x.shape\n",
    "        Pred = batch_y.shape[1]\n",
    "\n",
    "        dummy_mark_enc = torch.zeros(B, Seq, 4,  device=device)\n",
    "        dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "        dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "        outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "\n",
    "        f_dim = -1 if configs.c_out == 1 else 0\n",
    "        preds = outputs[:, -configs.pred_len:, f_dim:]  # (B, 10, C)\n",
    "        true  = batch_y\n",
    "\n",
    "        loss = criterion(preds, true)\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        current_loss = loss.item() * ACCUM_STEPS\n",
    "        total_loss  += current_loss\n",
    "        current_lr   = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix({'loss': f\"{current_loss:.5f}\", 'lr': f\"{current_lr:.6f}\"})\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader_global)\n",
    "    print(f\"[Epoch {epoch}] Avg Loss: {avg_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4733253-3521-4d7d-8107-f4db33be1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. 모델 저장\n",
    "# ============================================================\n",
    "SAVE_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, \"ship_time_llm_tmp4.pth\")\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\n[SAVE] Model saved to: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23e9392-3137-43ef-a39b-8bca44aa2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. 평가 함수 (MSE / DIR%) + Horizon 분석\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "def eval_loader(loader, name=\"train\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    mse_model_list = []\n",
    "    mse_naive_list = []\n",
    "    dir_model_list = []\n",
    "    dir_naive_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]  # (B, Pred, C)\n",
    "\n",
    "            true = batch_y[:, :, 0]   # (B, Pred)\n",
    "            pred = preds[:, :, 0]     # (B, Pred)\n",
    "\n",
    "            # 1) MSE\n",
    "            mse_model = torch.mean((pred - true)**2).item()\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)\n",
    "            mse_naive = torch.mean((naive - true)**2).item()\n",
    "\n",
    "            mse_model_list.append(mse_model)\n",
    "            mse_naive_list.append(mse_naive)\n",
    "\n",
    "            # 2) 방향 정확도\n",
    "            true_ret = true[:, 1:] - true[:, :-1]\n",
    "            pred_ret = pred[:, 1:] - pred[:, :-1]\n",
    "\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            mask = true_sign != 0\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            acc_m = (pred_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "            acc_n = (naive_sign[mask] == true_sign[mask]).float().mean().item()\n",
    "\n",
    "            dir_model_list.append(acc_m)\n",
    "            dir_naive_list.append(acc_n)\n",
    "\n",
    "    avg_mse_model = np.mean(mse_model_list)\n",
    "    avg_mse_naive = np.mean(mse_naive_list)\n",
    "    avg_dir_model = np.mean(dir_model_list) * 100\n",
    "    avg_dir_naive = np.mean(dir_naive_list) * 100\n",
    "\n",
    "    print(f\"[{name}] MSE   model={avg_mse_model:.4f}, naive={avg_mse_naive:.4f}\")\n",
    "    print(f\"[{name}] DIR%  model={avg_dir_model:.2f}%, naive={avg_dir_naive:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def eval_horizon(loader, name=\"val\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    Pred = configs.pred_len\n",
    "\n",
    "    mse_model_h = [[] for _ in range(Pred)]\n",
    "    mse_naive_h = [[] for _ in range(Pred)]\n",
    "    dir_model_h = [[] for _ in range(Pred-1)]\n",
    "    dir_naive_h = [[] for _ in range(Pred-1)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "\n",
    "            B, Seq, C = batch_x.shape\n",
    "            Pred = batch_y.shape[1]\n",
    "\n",
    "            dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "            dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "            dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "            outputs = model(batch_x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            f_dim = -1 if configs.c_out == 1 else 0\n",
    "            preds = outputs[:, -configs.pred_len:, f_dim:]\n",
    "\n",
    "            true = batch_y[:, :, 0]\n",
    "            pred = preds[:, :, 0]\n",
    "            naive = batch_x[:, -1, 0].unsqueeze(1).repeat(1, Pred)\n",
    "\n",
    "            # Horizon별 MSE\n",
    "            for h in range(Pred):\n",
    "                mse_m = torch.mean((pred[:, h] - true[:, h])**2).item()\n",
    "                mse_n = torch.mean((naive[:, h] - true[:, h])**2).item()\n",
    "                mse_model_h[h].append(mse_m)\n",
    "                mse_naive_h[h].append(mse_n)\n",
    "\n",
    "            # Horizon별 방향 정확도\n",
    "            true_ret  = true[:, 1:] - true[:, :-1]\n",
    "            pred_ret  = pred[:, 1:] - pred[:, :-1]\n",
    "            true_sign = torch.sign(true_ret)\n",
    "            pred_sign = torch.sign(pred_ret)\n",
    "\n",
    "            last_hist_ret = batch_x[:, -1, 0] - batch_x[:, -2, 0]\n",
    "            naive_sign = torch.sign(last_hist_ret).unsqueeze(1).repeat(1, Pred-1)\n",
    "\n",
    "            for h in range(Pred-1):\n",
    "                ts = true_sign[:, h]\n",
    "                ps = pred_sign[:, h]\n",
    "                ns = naive_sign[:, h]\n",
    "\n",
    "                mask = ts != 0\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                acc_m = (ps[mask] == ts[mask]).float().mean().item()\n",
    "                acc_n = (ns[mask] == ts[mask]).float().mean().item()\n",
    "\n",
    "                dir_model_h[h].append(acc_m)\n",
    "                dir_naive_h[h].append(acc_n)\n",
    "\n",
    "    mse_model_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_model_h]\n",
    "    mse_naive_h = [np.mean(v) if len(v) > 0 else np.nan for v in mse_naive_h]\n",
    "    dir_model_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_model_h]\n",
    "    dir_naive_h = [np.mean(v)*100 if len(v) > 0 else np.nan for v in dir_naive_h]\n",
    "\n",
    "    print(f\"== [{name}] Horizon별 MSE (h=0은 +1일차) ===\")\n",
    "    for h in range(Pred):\n",
    "        print(f\"h+{h+1}: MSE_model={mse_model_h[h]:.4f}, MSE_naive={mse_naive_h[h]:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== [{name}] Horizon별 방향 정확도 (1~9일 구간) ===\")\n",
    "    for h in range(Pred-1):\n",
    "        print(f\"구간 {h+1}->{h+2}: DIR_model={dir_model_h[h]:.2f}%, DIR_naive={dir_naive_h[h]:.2f}%\")\n",
    "\n",
    "    return mse_model_h, mse_naive_h, dir_model_h, dir_naive_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97525035-412b-4544-915f-bfd049a7d08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(0.04889419426788287),\n",
       "  np.float64(0.08704641057531638),\n",
       "  np.float64(0.11911700773534589),\n",
       "  np.float64(0.15280917278844222),\n",
       "  np.float64(0.18529037463253942),\n",
       "  np.float64(0.22620725822795568),\n",
       "  np.float64(0.24720519549887757),\n",
       "  np.float64(0.2849396393024202),\n",
       "  np.float64(0.3113947001826279),\n",
       "  np.float64(0.349485604259324)],\n",
       " [np.float64(0.030715870161155432),\n",
       "  np.float64(0.06035709488486614),\n",
       "  np.float64(0.09029256207627598),\n",
       "  np.float64(0.12113065682833307),\n",
       "  np.float64(0.1470897889862656),\n",
       "  np.float64(0.17020487708264384),\n",
       "  np.float64(0.19079851161624337),\n",
       "  np.float64(0.21216568321888818),\n",
       "  np.float64(0.2335972248672925),\n",
       "  np.float64(0.2584585758279367)],\n",
       " [np.float64(48.481117394463766),\n",
       "  np.float64(48.16297274725191),\n",
       "  np.float64(52.77093654048854),\n",
       "  np.float64(48.871100873782716),\n",
       "  np.float64(50.882595289370116),\n",
       "  np.float64(55.377669067218385),\n",
       "  np.float64(51.919130492826994),\n",
       "  np.float64(46.089902186188205),\n",
       "  np.float64(49.13793193369076)],\n",
       " [np.float64(51.39573144501654),\n",
       "  np.float64(43.78078877925873),\n",
       "  np.float64(45.94622398244923),\n",
       "  np.float64(47.711412865540076),\n",
       "  np.float64(45.925698429346085),\n",
       "  np.float64(49.10714356549855),\n",
       "  np.float64(46.5209365661802),\n",
       "  np.float64(49.517652597920645),\n",
       "  np.float64(49.44581350889699)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. 평가 실행 예시\n",
    "# ============================================================\n",
    "print(\"\\n[Eval] Global 성능 평가 시작\")\n",
    "eval_loader(train_loader_global, \"train\")\n",
    "eval_loader(val_loader_global,   \"val\")\n",
    "eval_horizon(val_loader_global,  \"val\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
