{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70e4b5d-6aae-4505-9973-1e62e6baf047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. 기본 import + 디바이스 설정\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84da99bc-83b0-46e3-a354-be59c6c9da78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT: /workspace/ship-ai\n",
      "[INFO] DATA_DIR    : /workspace/ship-ai/data/processed\n",
      "[INFO] MASTER_TBL  : /workspace/ship-ai/data/processed/final_master_table_v2.csv\n",
      "[INFO] GPT2_PATH   : /workspace/ship-ai/pretrained_models/gpt2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# [1] --- DataHandler 클래스 정의 (V2: Scaler + zfill) ---\n",
    "# (Phase 2-A와 2-C가 합쳐진 최종 버전)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ============================================================\n",
    "# 2. 경로 설정 (프로젝트 구조에 맞게 필요시 수정)\n",
    "# ============================================================\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "FINAL_MASTER_FILE = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "MASTER_TABLE_PATH = os.path.join(DATA_DIR, \"final_master_table_v2.csv\")\n",
    "\n",
    "GPT2_PATH       = os.path.join(PROJECT_ROOT, \"pretrained_models\", \"gpt2\")\n",
    "TIME_LLM_ROOT   = os.path.join(PROJECT_ROOT, \"external\", \"time-llm\")\n",
    "\n",
    "if TIME_LLM_ROOT not in sys.path:\n",
    "    sys.path.append(TIME_LLM_ROOT)\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    [V2] 표준화(Standardization)와 zfill(6)이 적용된 DataHandler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, train_end_date='2022-12-31'):\n",
    "        self.file_path = file_path\n",
    "        self.train_end_date = pd.to_datetime(train_end_date)\n",
    "        self.data_by_ticker = {}   # 원본 데이터\n",
    "        self.scalers_by_ticker = {} # Ticker별 Scaler\n",
    "        self.tickers = []\n",
    "        \n",
    "        self._load_and_process_data()\n",
    "        self._fit_scalers()\n",
    "        \n",
    "    def _load_and_process_data(self):\n",
    "        try:\n",
    "            # 1. dtype=str로 읽기\n",
    "            df = pd.read_csv(\n",
    "                self.file_path, \n",
    "                parse_dates=['date'],\n",
    "                dtype={'ticker': str} \n",
    "            )\n",
    "            # 2. zfill(6)로 '0' 채우기\n",
    "            df['ticker'] = df['ticker'].str.zfill(6)\n",
    "            df = df.set_index('date')\n",
    "            \n",
    "            self.tickers = df['ticker'].unique()\n",
    "            \n",
    "            for ticker in self.tickers:\n",
    "                ticker_df = df[df['ticker'] == ticker].copy()\n",
    "                channel_cols = [col for col in ticker_df.columns if col not in ['ticker']]\n",
    "                self.data_by_ticker[ticker] = ticker_df[channel_cols]\n",
    "            \n",
    "            print(f\"[DataHandler V2] Success: Loaded {len(self.tickers)} tickers.\")\n",
    "            print(f\"[DataHandler V2] Available tickers: {self.tickers}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DataHandler V2] Error loading data: {e}\")\n",
    "\n",
    "    def _fit_scalers(self):\n",
    "        \"\"\"\n",
    "        [Data Leakage 방지] 훈련 데이터로만 Scaler를 학습(fit)\n",
    "        \"\"\"\n",
    "        print(f\"[DataHandler V2] Fitting scalers using data up to {self.train_end_date.date()}...\")\n",
    "        for ticker in self.tickers:\n",
    "            train_data = self.data_by_ticker[ticker].loc[:self.train_end_date]\n",
    "            if train_data.empty:\n",
    "                print(f\"  > Warning: No training data for {ticker}.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_data) # 'fit'은 훈련 데이터로만!\n",
    "            self.scalers_by_ticker[ticker] = scaler\n",
    "        print(\"[DataHandler V2] Scalers fitted.\")\n",
    "\n",
    "    def get_scaled_data_by_ticker(self, ticker):\n",
    "        \"\"\"\n",
    "        'transform'은 전체 데이터에 적용하여 표준화된 DF 반환\n",
    "        \"\"\"\n",
    "        if ticker not in self.scalers_by_ticker:\n",
    "            print(f\"[DataHandler V2] Error: No scaler for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        original_data = self.data_by_ticker[ticker]\n",
    "        scaler = self.scalers_by_ticker[ticker]\n",
    "        \n",
    "        scaled_data_np = scaler.transform(original_data)\n",
    "        \n",
    "        scaled_df = pd.DataFrame(\n",
    "            scaled_data_np, \n",
    "            index=original_data.index, \n",
    "            columns=original_data.columns\n",
    "        )\n",
    "        return scaled_df\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        return self.tickers\n",
    "\n",
    "print(\"[INFO] PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"[INFO] DATA_DIR    :\", DATA_DIR)\n",
    "print(\"[INFO] MASTER_TBL  :\", MASTER_TABLE_PATH)\n",
    "print(\"[INFO] GPT2_PATH   :\", GPT2_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80c9180-e646-42fc-85ee-f669d2dc484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TimeLLM 모델 임포트 성공\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. TimeLLM 모델 import\n",
    "# ============================================================\n",
    "try:\n",
    "    import importlib\n",
    "    import models.TimeLLM\n",
    "    importlib.reload(models.TimeLLM)\n",
    "    from models.TimeLLM import Model as TimeLLM\n",
    "    print(\"[INFO] TimeLLM 모델 임포트 성공\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] TimeLLM import 실패:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa4386e-c0fe-499c-af17-ecac274a9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. 슬라이딩 윈도우 함수 + Dataset 정의\n",
    "# ============================================================\n",
    "def create_sliding_windows(data, input_seq_len, output_seq_len):\n",
    "    \"\"\"\n",
    "    DataFrame(2D: [time, features]) -> (X, y) 3D numpy 배열로 변환\n",
    "    X: (N, input_seq_len, C)\n",
    "    y: (N, output_seq_len, C)\n",
    "    \"\"\"\n",
    "    data_np = data.values\n",
    "    n_samples = len(data_np)\n",
    "    X, y = [], []\n",
    "\n",
    "    total_len = input_seq_len + output_seq_len\n",
    "    for i in range(n_samples - total_len + 1):\n",
    "        x_win = data_np[i : i + input_seq_len]\n",
    "        y_win = data_np[i + input_seq_len : i + total_len]\n",
    "        X.append(x_win)\n",
    "        y.append(y_win)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.Y = torch.FloatTensor(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1274930-a5f0-4eec-8677-13aac0b618c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        # 기본 세팅\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'Stock_Prediction'\n",
    "        self.model = 'TimeLLM'\n",
    "\n",
    "        # 데이터 차원\n",
    "        self.seq_len   = 120\n",
    "        self.label_len = 60\n",
    "        self.pred_len  = 10\n",
    "        self.enc_in = 12\n",
    "        self.dec_in = 12\n",
    "        self.c_out = 12\n",
    "\n",
    "        # LLM 설정 (학습 때와 동일하게!)\n",
    "        self.llm_model      = 'GPT2'\n",
    "        self.llm_model_path = GPT2_PATH\n",
    "        self.llm_dim    = 768\n",
    "        self.llm_layers = 8\n",
    "\n",
    "        # Patch 설정\n",
    "        self.patch_len = 8\n",
    "        self.stride    = 4\n",
    "\n",
    "        # 모델 차원\n",
    "        self.d_model = 512\n",
    "        self.d_ff    = 512\n",
    "        self.n_heads = 12\n",
    "        self.dropout = 0.00\n",
    "\n",
    "        # Prompt\n",
    "        self.prompt_domain = 1\n",
    "        self.content = (\n",
    "            \"Task: Forecast daily closing prices for Korean shipbuilding companies. \"\n",
    "            \"Input Data: 12 channels including OHLC prices, trading volume, \"\n",
    "            \"and macro-indicators such as Brent oil price, USD/KRW exchange rate, \"\n",
    "            \"interest rate, and BDI (Baltic Dry Index). \"\n",
    "            \"Context: Shipbuilding stocks are sensitive to oil prices and BDI. \"\n",
    "            \"Analyze the 120-day trend, focusing on volatility and correlations, \"\n",
    "            \"and predict the next 10 days.\"\n",
    "        )\n",
    "\n",
    "        # 기타\n",
    "        self.embed   = 'timeF'\n",
    "        self.freq    = 'd'\n",
    "        self.factor  = 1\n",
    "        self.moving_avg = 25\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.top_k    = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3b308a-dbc1-4357-85b9-50e3dcd7028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loaded weights from /workspace/ship-ai/models/ship_time_llm_tmp6_ft_h3_e5.pth\n",
      "✅ Loaded model from: /workspace/ship-ai/models/ship_time_llm_tmp6_ft_h3_e5.pth\n"
     ]
    }
   ],
   "source": [
    "configs = Configs()\n",
    "model = TimeLLM(configs).to(device).float()\n",
    "\n",
    "SAVE_PATH = os.path.join(PROJECT_ROOT, \"models\", \"ship_time_llm_tmp6_ft_h3_e5.pth\")\n",
    "state = torch.load(SAVE_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "print(\"[LOAD] Loaded weights from\", SAVE_PATH)\n",
    "\n",
    "# 기존보다 살짝 낮은 lr로\n",
    "LEARNING_RATE = 3e-5\n",
    "ACCUM_STEPS = 8\n",
    "EPOCHS = 8\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"✅ Loaded model from:\", SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1f5727-6923-4ab7-b2fb-994bef7aaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataHandler V2] Success: Loaded 6 tickers.\n",
      "[DataHandler V2] Available tickers: ['010140' '010620' '329180' '042660' '443060' '009540']\n",
      "[DataHandler V2] Fitting scalers using data up to 2022-12-31...\n",
      "  > Warning: No training data for 329180.\n",
      "  > Warning: No training data for 443060.\n",
      "[DataHandler V2] Scalers fitted.\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(MASTER_TABLE_PATH, train_end_date='2022-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106ddf56-f42a-44a4-8c7c-f8a594e9c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_time_llm_forecast(ticker: str, as_of_date: str, tau: float = -0.0975):\n",
    "    \"\"\"\n",
    "    Time-LLM V1 엔진을 사용해\n",
    "      - 입력: ticker(문자열 6자리), 기준일(as_of_date, \"YYYY-MM-DD\")\n",
    "      - 출력: 10일 예측 경로 + 누적 수익률 + 실제 가격 경로 등\n",
    "    을 반환하는 헬퍼 함수.\n",
    "\n",
    "    전제:\n",
    "      - data_handler: DataHandler 인스턴스\n",
    "      - configs    : Configs 인스턴스 (seq_len=120, pred_len=10)\n",
    "      - model      : TimeLLM(configs) + weight 로딩 완료\n",
    "      - device     : torch.device(...)\n",
    "      - DataHandler는 close_log까지 StandardScaler로 표준화된 상태\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    seq_len  = configs.seq_len\n",
    "    pred_len = configs.pred_len\n",
    "\n",
    "    # 1) 날짜 / 스케일된 데이터 가져오기\n",
    "    as_of_ts = pd.to_datetime(as_of_date)\n",
    "\n",
    "    df_scaled = data_handler.get_scaled_data_by_ticker(ticker)\n",
    "    df_scaled = df_scaled.sort_index()\n",
    "\n",
    "    if len(df_scaled) < seq_len:\n",
    "        raise ValueError(f\"[run_time_llm_forecast] ticker {ticker} 전체 길이가 {len(df_scaled)}로 \"\n",
    "                         f\"seq_len={seq_len}보다 짧습니다.\")\n",
    "\n",
    "    # as_of_date가 영업일이 아닐 수 있으니, 그 이전 가장 가까운 날짜로 맞춰주기\n",
    "    if as_of_ts not in df_scaled.index:\n",
    "        candidates = df_scaled.index[df_scaled.index <= as_of_ts]\n",
    "        if len(candidates) == 0:\n",
    "            raise ValueError(f\"[run_time_llm_forecast] {as_of_date} 이전 데이터가 없습니다.\")\n",
    "        as_of_ts = candidates[-1]  # 가장 최근 영업일\n",
    "\n",
    "    idx = df_scaled.index.get_loc(as_of_ts)\n",
    "\n",
    "    if idx + 1 < seq_len:\n",
    "        raise ValueError(\n",
    "            f\"[run_time_llm_forecast] {ticker} @ {as_of_ts.date()} 기준으로 \"\n",
    "            f\"필요한 과거 {seq_len}일이 부족합니다. (현재 {idx+1}일)\"\n",
    "        )\n",
    "\n",
    "    start = idx - seq_len + 1\n",
    "    end   = idx + 1               # iloc에서 end는 exclusive, 그래서 +1\n",
    "    window_scaled = df_scaled.iloc[start:end].values  # (seq_len, C)\n",
    "\n",
    "    # 2) 텐서 변환\n",
    "    x = torch.FloatTensor(window_scaled).unsqueeze(0).to(device)  # (1, seq_len, C)\n",
    "    B, Seq, C = x.shape\n",
    "    Pred = pred_len\n",
    "\n",
    "    dummy_mark_enc = torch.zeros(B, Seq, 4, device=device)\n",
    "    dummy_mark_dec = torch.zeros(B, Pred, 4, device=device)\n",
    "    dummy_dec_in   = torch.zeros(B, Pred, C, device=device)\n",
    "\n",
    "    # 3) 모델 추론 (스케일된 공간)\n",
    "    with torch.no_grad():\n",
    "        out = model(x, dummy_mark_enc, dummy_dec_in, dummy_mark_dec)\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        preds_scaled = out[:, -pred_len:, :]  # (1, Pred, C)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 스케일된 close_log 기반 정보\n",
    "    # -----------------------------\n",
    "    pred_close_scaled = preds_scaled[0, :, 0].cpu().numpy()  # (Pred,)\n",
    "    last_close_scaled = x[0, -1, 0].cpu().item()\n",
    "    prev_close_scaled = x[0, -2, 0].cpu().item()\n",
    "\n",
    "    # (1) \"학습 시 사용하던\" CUM10 (표준화 공간)\n",
    "    pred_cum10_scaled = float(pred_close_scaled[-1] - last_close_scaled)\n",
    "    last_ret_scaled   = last_close_scaled - prev_close_scaled\n",
    "    naive_cum10_scaled = float(last_ret_scaled * Pred)\n",
    "\n",
    "    # τ 기준으로 방향 레이블 (학습 시와 동일한 공간에서 판정)\n",
    "    direction_label = \"UP_or_NEUTRAL\" if pred_cum10_scaled > tau else \"DOWN_RISK\"\n",
    "\n",
    "    # (2) 스케일된 경로에서 일별 수익률 (참고용)\n",
    "    pred_ret_scaled_path = (pred_close_scaled[1:] - pred_close_scaled[:-1]).tolist()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) StandardScaler 역변환 (denorm)\n",
    "    # -----------------------------\n",
    "    scaler = data_handler.scalers_by_ticker[ticker]\n",
    "\n",
    "    # 마지막 두 개 관측값 (전부 채널 포함) 역변환\n",
    "    last_two_scaled = x[0, -2:, :].cpu().numpy()    # (2, C)\n",
    "    last_two_denorm = scaler.inverse_transform(last_two_scaled)  # (2, C)\n",
    "    prev_full_denorm = last_two_denorm[0]  # (C,)\n",
    "    last_full_denorm = last_two_denorm[1]  # (C,)\n",
    "\n",
    "    prev_close_log_denorm = float(prev_full_denorm[0])  # 0번 채널: close_log\n",
    "    last_close_log_denorm = float(last_full_denorm[0])\n",
    "\n",
    "    # 예측 경로 전체 역변환\n",
    "    preds_scaled_np   = preds_scaled[0].cpu().numpy()            # (Pred, C)\n",
    "    preds_denorm_full = scaler.inverse_transform(preds_scaled_np)  # (Pred, C)\n",
    "    pred_close_log_denorm = preds_denorm_full[:, 0]              # (Pred,)\n",
    "\n",
    "    # log(price) → price 복원 (close_log = log(price) 라는 가정)\n",
    "    last_price = float(np.exp(last_close_log_denorm))\n",
    "    prev_price = float(np.exp(prev_close_log_denorm))\n",
    "    pred_price_path = np.exp(pred_close_log_denorm)              # (Pred,)\n",
    "\n",
    "    # 10일 log-return / price-return\n",
    "    pred_cum10_log  = float(pred_close_log_denorm[-1] - last_close_log_denorm)\n",
    "    naive_cum10_log = float((last_close_log_denorm - prev_close_log_denorm) * Pred)\n",
    "\n",
    "    pred_cum10_return  = float(pred_price_path[-1] / last_price - 1.0)\n",
    "    naive_cum10_return = float((last_price / prev_price) ** Pred - 1.0)\n",
    "\n",
    "    result = {\n",
    "        \"ticker\": ticker,\n",
    "        \"as_of_date\": as_of_ts.strftime(\"%Y-%m-%d\"),\n",
    "        \"pred_horizon_days\": Pred,\n",
    "        \"tau_threshold_scaled\": tau,\n",
    "\n",
    "        # --- 모델 학습 시 사용하던 \"표준화 공간\" 기준 정보 ---\n",
    "        \"scaled_space\": {\n",
    "            \"last_close\": last_close_scaled,\n",
    "            \"pred_path_close\": pred_close_scaled.tolist(),\n",
    "            \"pred_ret_path\": pred_ret_scaled_path,\n",
    "            \"pred_cum10\": pred_cum10_scaled,\n",
    "            \"naive_cum10\": naive_cum10_scaled,\n",
    "            \"direction_label\": direction_label,\n",
    "        },\n",
    "\n",
    "        # --- 실제 feature 공간 (denorm) ---\n",
    "        \"denorm\": {\n",
    "            \"last_close_log\": last_close_log_denorm,\n",
    "            \"prev_close_log\": prev_close_log_denorm,\n",
    "            \"pred_close_log_path\": pred_close_log_denorm.tolist(),\n",
    "\n",
    "            \"last_close_price\": last_price,\n",
    "            \"prev_close_price\": prev_price,\n",
    "            \"pred_price_path\": pred_price_path.tolist(),\n",
    "\n",
    "            \"pred_cum10_log\": pred_cum10_log,\n",
    "            \"naive_cum10_log\": naive_cum10_log,\n",
    "            \"pred_cum10_return\": pred_cum10_return,      # ≈ 10일 수익률\n",
    "            \"naive_cum10_return\": naive_cum10_return,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2295bb05-f5b7-41e7-9edb-b9698d7692fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ticker', 'as_of_date', 'pred_horizon_days', 'tau_threshold_scaled', 'scaled_space', 'denorm']),\n",
       " 'UP_or_NEUTRAL',\n",
       " 0.08260738849639893)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = run_time_llm_forecast(\"010140\", \"2023-06-30\")  # 있는 날짜 대충 하나\n",
    "res.keys(), res[\"scaled_space\"][\"direction_label\"], res[\"denorm\"][\"pred_cum10_return\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a03caf-cb90-46f2-ac06-03fd0484b95a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 필요하면\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# 필요하면\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
